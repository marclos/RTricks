\documentclass{article}
\usepackage{natbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=purple,
    citecolor=violet
}
%\usepackage{comment}
\usepackage{xurl}
\usepackage[dvipsnames]{xcolor}
\colorlet{LightRubineRed}{RubineRed!100}
\definecolor{mypink2}{RGB}{219, 48, 122}

\title{Communicating Climate Trends}
\author{Marc Los Huertos}
\date{\today~(ver. 0.08)} 

\begin{document}
\maketitle

\section{Introduction}

Developing the skills to communicate climate trends is an important skill for scientists and policy makers. This exercise will focus on developing the skills to communicate climate trends using R and RMarkdown. We'll produce graphics that can be used in the video presentations.

\subsection{Goals}

Create a compelling story\dots 

\subsection{Approach}

\subsubsection{R Code with Custom Functions}

From the Canvas page, go to the Guide2functions.R file and download the file to your computer. Then upload the file to Rstudio directory you are using for the project.

Open the file in Rstudio and run the code, using the ``source''. button near the top of the editor window.

Run the \href{https://github.com/marclos/RTricks/blob/master/04_Regional_Climate_Trends/Guides/Guide4functions.R}{Guide4functions.R} code and the functions will be loaded into your environment automatically.


<<setup, echo=FALSE, results='hide', message=FALSE>>=
datafolder = "/home/mwl04747/RTricks/04_Regional_Climate_Trends/Data/SP24/"
library(xtable)
library(here)
source(here("04_Regional_Climate_Trends", "Guides", "Guide4functions.R"))
@

As we get further into the project, these code chunks may require tweaking because of the questions you are interested for your location falls outside the design of the custom functions. Please let Marc or the mentors know to get assistance tweaking the code!

\section{Mapping (GIS) and Weather Stations}

\subsection{Simple Mapping of Stations}

\subsection{More Complex Mapping of Stations}



<<eval=FALSE>>=
library(geodata)
d <- worldclim_country(country = "USA", var = "tmin",
                       path = tempdir())
terra::plot(mean(d), plg = list(title = "Min. temperature (C)"))
@

<<>>=
library(here)
library(xtable)

stations.active.oldest = read.csv(
  here("04_Regional_Climate_Trends", "stations.active.oldest.csv"))

# OR
# use file.choose() to select the file
# filename = "MY.PATH/04_Regional_Climate_Trends/stations.active.oldest.csv"
# stations.active.oldest = read.csv(filename)
@ 


\subsection{Map US Weather Stations}

Here's map that has been transformed using a bunch of libraries that I don't really know how to use, but I found webpage that told me how to make this!

It would be nice to add Canada and Mexico so the USA is not floating in space. I'll work on that later.

<<>>=
library(usmap)
library(ggplot2)
library(sf)
library(tidyverse)
library("rnaturalearth")
library("rnaturalearthdata")
library(usmap)

world <- ne_countries(scale = "medium", returnclass = "sf")
class(world)

usa <- subset(world, admin == "United States of America")

(mainland <- ggplot(data = usa) +
     geom_sf(fill = "cornsilk") +
     #geom_sf(data = subset(sites), size = 3, shape = 21, fill = "gray70") +
     geom_sf(data = subset(sites.2163), size = 1, shape = 21, fill = "red") +
    coord_sf(crs = st_crs(2163), xlim = c(-2500000, 2500000), ylim = c(-2300000, 
         730000)))

(alaska <- ggplot(data = usa) +
     geom_sf(fill = "cornsilk") +
    geom_sf(data = subset(sites.2163), size = 1, shape = 21, fill = "red") +
     coord_sf(crs = st_crs(3467), xlim = c(-2400000, 1600000), ylim = c(200000, 
         2500000), expand = FALSE, datum = NA))

(hawaii  <- ggplot(data = usa) +
     geom_sf(fill = "cornsilk") +
     geom_sf(data = subset(sites.2163), size = 1, shape = 21, fill = "red") +
     coord_sf(crs = st_crs(4135), xlim = c(-161, -154), ylim = c(18, 
         23), expand = FALSE, datum = NA))

(world.sf <- ggplot(data =world) +
     geom_sf() +
     #geom_sf(data = subset(sites), size = 3, shape = 21, fill = "gray70") +
     geom_sf(data = subset(sites.2163), size = 1, shape = 21, fill = "red") +
    coord_sf(crs = st_crs(2163), xlim = c(-2500000, 2500000), ylim = c(-2300000, 
         730000)))


mainland +
 annotation_custom(
      grob = ggplotGrob(alaska),
      xmin = -2750000,
      xmax = -2750000 + (1600000 - (-2400000))/2.5,
      ymin = -2450000,
      ymax = -2450000 + (2500000 - 200000)/2.5
  ) +
  annotation_custom(
      grob = ggplotGrob(hawaii),
      xmin = -1250000,
      xmax = -1250000 + (-154 - (-161))*120000,
      ymin = -2450000,
      ymax = -2450000 + (23 - 18)*120000
  )

@


\section{Communicating Long-term Weather Records}

\subsection{Creating A Basic Trend Plot}

begin{description}

\item[Creating a Basic Trend Plot] The first step in analyzing the data is to create a basic trend plot. This plot will show the long-term trend in the data.

The function requires two inputs, the station and the data element (e.g., temperature, precipitation, etc.). The function will then create a basic trend plot for the station and data element.

\begin{center}
\textbf{Function: \textcolor{Plum}{BasicTrend.fun()}}
\end{center}


Example of how to use the function:

<<echo=TRUE, results='hide'>>=
station = "USW00013739.anomalies"
month = 6
element = "TMAX"
BasicTrendPlot.fun(station, month, element)
@


<<>>=
par(mar=c(5,5,5,5), las=1)
      
plot(TMAX.a ~ Ymd, data=subset(USC00042294.anomalies$TMAX, 
    MONTH==6), las=1, pch=20, cex=.5, col="grey", 
    ylab="Temp Anamoly (°C) ", 
    main="", 
    #sub="For Station USC00042294, 1893-2012, slope = 0.001,  p-value < 0.001, r2 = 0.02", 
    xlab="Year", xlim=c(as.Date("1850-01-01"), as.Date("2050-01-01")), 
    frame.plot=FALSE)
mtext("Maximum Daily Temperature Trend for June", cex=1.3, side=3, line=2)
mtext("Trend Line: 0.001/100 yrs; p-value < 0.001, r2 = 0.02", side=3, line=1, cex=1.0, col="red")

USC00042294.lm = lm(TMAX.a ~ Ymd, 
          data=subset(USC00042294.anomalies$TMAX, MONTH==6))

abline(coef(USC00042294.lm), col="red")
@

\subsection{Complete Records vs. Post 1975 Trends}

Communicating climate change based on station records is tricky. The long-term record would on the surface to be the most robust, but several issues arise with a naive analytical approach -- my favorite!


<<trend1975, eval=FALSE>>=

GSOM_1975.png = paste0(fips$State2, "_", stid, "_GSOM_1975.png")

png(paste0(png_private, GSOM_1975.png), width = 480, height = 320, units = "px", pointsize = 12, bg = "white")
par(las=1, mfrow=c(1,1))
plot(TMAX~Date, GSOM, pch=20, cex=.5, col="grey", ylab="°F", main=paste0(fips$State2, "-", stid))
GSOM.lm = lm(TMAX~Date, GSOM)
pred_dates <-data.frame(Date = GSOM$Date); 
nrow(pred_dates);# pred_dates
#Predits the values with confidence interval 
ci <- predict(GSOM.lm, newdata = pred_dates, 
              interval = 'confidence')
lines(pred_dates$Date, as.numeric(ci[,1]), col="gray50")

# Post 1975
GSOM.lm = lm(TMAX~Date, GSOM[GSOM$Year>1975,])
pred_dates <-data.frame(Date = GSOM$Date[GSOM$Year>1975]); 
nrow(pred_dates); #pred_dates
#Predits the values with confidence interval 
ci <- predict(GSOM.lm, newdata = pred_dates, 
              interval = 'confidence')
lines(pred_dates$Date, as.numeric(ci[,1]), col="red")
lines(pred_dates$Date, as.numeric(ci[,2]), col="darkorange")
lines(pred_dates$Date, ci[,3], col="darkorange")
location_index = round(length(GSOM[GSOM$Year>1975,]$Date) * 0.99,0)
text(pred_dates$Date[location_index], ci[location_index,3], 
     paste(report_prob3(GSOM.lm))[2], pos=2, cex=1.0, col="red")
#abline(coef(lm(TMAX~Date, GSOM)), col="black")
#abline(coef(lm(TMAX~Date, GSOM[GSOM$Year>1975,])), col="red")
dev.off()
@

The noise in the data may suggests that no trend is present %(Figure~\ref{fig:GSOM-1975trend}). It's tricky because the seasonal variation dominates the source of varition. In other words the intra-annual variation exceeds the inter-annual variation, making signal detection very difficult. Is there a way to "filter" out the seasonal effect? Yes, let's see how that works next. 

%\begin{figure}
%\includegraphics[width=1.00\textwidth]{\Sexpr{paste0(png_private, GSOM_1975.png)}}
%\caption{The climate trends from full record and post 1975 data. These data have a high level of variability due to seasonality effects that have not been filtered out%.}
%\label{fig:GSOM-1975trend}
%\end{figure}



\section{Extreme Events--Using Daily Records}

\subsection{Complicated Nature of Rainfall Patterns}

Rainfall trends are tough. Exteme events can occur in 24 hours or over long periods that might result in floods or droughts. Each region might have different patterns, so developing a consistent approach is tough.

We can look for trends in monthly averages, number of days without rain (important in tropics), and/or extreme events based on daily or hourly data. 

I don't know of a robust way to look at this for the entire globe. 

Rainfall totals by season might be a useful way to think about changes, because the rainfall is often seasonal, I wonder if we can see pattners by season. 

<<ggplot, warning=FALSE, eval=FALSE>>=
ggplot( ) +
   geom_bar(data = PRCP.Season.Total, 
      aes(x=Year, y=PRCP, fill=Season), stat="identity") + 
         xlim(min(CHCND$Year), max(CHCND$Year)-1) +
   #ylab("Number of Extreme Temps") + # for the y axis label
   geom_smooth(data = PRCP.Total, 
      aes(y=PRCP, x=Year), method = "lm", 
      se = T, color= "black") 

# + geom_smooth(data= PRCP.Season.Total, aes(x=Year, y = PRCP, color = Season, group=Season), se=F)
@

\subsection{Drought}

Days without rain...within a calendar year... bleed over between years isn't captured.. This is screwed up, Drought.run needs work.

<<echo=FALSE, eval=FALSE>>=


@

\subsection{Rainfall Probability Distributions by decade... to be developed.}

<<eval=FALSE>>=

@

<<eval=FALSE, echo=FALSE>>=
CHCND$Score <- floor_score(CHCND$Year)
@

\subsection{Record Setting Temperature Records}

In many cases, people seem to "feel" how temperature has been changing over time, and new records seem to capture the attention in the media. So, we'll create a updated record of maximum temperatures and display them. 

<<extremetempplot, echo=FALSE, results='hide', eval=FALSE>>=

@



This is a common way to communicate temperatures changes. I suspect we have a better sense of change when we notice "extreme" events...

<<eval=FALSE>>=
  
   
@


<<eval=FALSE>>=

@

I tried to use a for loop and in then statements and it was painfully slow, so I converted the data to a matrix that can be used by barplots with much more effeciency!

Create the matrix

@

The patterns of record temperatures often shows increasing number of new high temperature records  and fewer record low temperatures more recently, but as usual, it depends on the location (Figure~\ref{fig:Records}).

%\begin{figure}
%\caption{Daily temperatures that have been the highest on record (in red) and lowest on record (in blue). In some cases, climate change has created more records in the recent decades, while other stations seem don't show that trend.}
%\label{fig:Records}
%\end{figure}

\subsection{Iterate TMAX vs. Month Boxplots}

Evaluating the changes in TMAX and Monthly temperatures might be useful, but for now, I think it's hard to see the patterns. 

<<boxplots, eval=FALSE, echo=FALSE, results='hide'>>=

@


\subsection{Four Plots Compelling Figures}

To test the code, I have created graphics that can then be used in the animation process, i.e. try to create code that doesn't get too complicated and then fail! 

<<static_template, eval=FALSE, echo=FALSE>>=

@

%\begin{figure}
%\includegraphics[width=1.00\textwidth]{\Sexpr{paste0(png_private, panel4.png)}}
%\caption{Climate can be analyzed using several types of lenses. In this case, we have analyzed show the months with the greatest changes. The first figure is monthly average of TMINs (daily low temperatures) with a best fit line. The second figure shows the monthly TMAX range and asterisks indicate singificant changes over the station record and the third figure is the trend for these TMAXs over time and includes the best fit line. The final figure shows the daily temperatures that have been the highest on record (in red) and the lowest minimum temperatures (in blue). In some cases, climate change has created more records in the recent decades, while other stations seem don't show that trend.}
%\label{fig:4panel}
%\end{figure}

\subsection{KISS}

Keeping it simple is critical in communicating scientific information. In this section, I try to come up with a consistent message for every state and a simple graphic. 

\subsubsection{Change Point Analysis}
First, TMIN and TMAX and change point analysis...

https://cran.r-project.org/web/packages/mcp/readme/README.html

<<eval=FALSE>>=


@

%\begin{figure}
%\includegraphics[width=1.00\textwidth]{\Sexpr{paste0(png_private, KISS.png)}}
%\caption{Keep it simple stupid!}
%\label{fig:GSOM-KISS}
%\end{figure}

\subsection{Temp \& Precipitation Probability}

To highlight the patterns of change, it might be useful to analyze how the probability ditributuion might change -- we can use a normal probability distribion as a theoretical distribution (and we can check if this distribuion is approrpriate with a Chi-Square test), or we can use the data to create a emperical distribution, which is my favored approach. 

I started with decade bins, but used 20 years bins (scores) to simplify the graphics while keeping a pretty good temporal resolution.

<<normalPDF>>=

@

This figure is pretty effective, but still needs work. 

%\begin{figure}
%\includegraphics[width=1.00\textwidth]{\Sexpr{paste0(png_private, GSOM_dnorm.png)}}
%\caption{The changing in monthly temperature data, assuming a normal probability distribution.}
%\label{fig:GSOM_dnorm}
%\end{figure}

\subsection{Using library densEstBayes}

Now, I used a screen split to look at the distribution of the temperate anomolies. First, we look at a simple histogram of the entire dataset. 

<<echo=FALSE, eval=FALSE>>=
par(mfrow=c(1,1))
hist(GSOM2$TMIN.anom, col = "gold",
     main = "", probability = TRUE, 
     xlab = "Minimum Temperature Anomaly (°F)")
@

The data center around zero, as expected, but are these normally distributed? 

%For TMAX there is a \Sexpr{round(shapiro.test(GSOM2$TMAX.anom)$p.value, 7)} probability that the distribution is the same as the normal distribution. For TMIN there is a \Sexpr{round(shapiro.test(GSOM2$TMIN.anom)$p.value, 7)} probability that the distribution is the same as the normal distribution. For PPT is a \Sexpr{round(shapiro.test(GSOM2$PPT.anom)$p.value, 7)} probability that the distribution is the same as the normal distribution.

<<shapiro_test, echo=FALSE, eval=FALSE>>=
if(shapiro.test(GSOM2$TMAX.anom)$p.value<.05 | 
   shapiro.test(GSOM2$TMIN.anom)$p.value<.05 | 
   shapiro.test(GSOM2$PPT.anom)$p.value<.05){
   text= "to avoid "
   } else {
   text="to use"
      }
@

%These values suggest that there is good reason \Sexpr{text} the normal probability distribution. 

Next we use a function to estimate the probability distribution using a markof chain the creates an estimated probability distribution. This doesn't always work when the distribution is not even and their only 10 years of data per slot. I suspect, I should make this by every 20 years. Plus that will go way faster and I think the data visualization will be more robust. 

<<estDensity, warning=FALSE, results='hide', echo=FALSE>>=
GSOM_estPDF.png = paste0(fips$State2, "_", stid, "_GSOM_estPDF.png")

if(!file.exists(paste0(png_private, GSOM_estPDF.png))){
   print("Creating estimated density distribution")

png(paste0(png_private, GSOM_estPDF.png), 
    width = 480, height = 320, units = "px", pointsize = 12, bg = "white")

# Split Screen TMAX Legend TMIN
# screen with values for left, right, bottom, and top.
split.screen(rbind(c(.01, 0.99, 0.86, 0.95),
                   c(0.01, 0.45, 0.01, 0.85),
                   c(0.45, 0.55, 0.01, 0.85),
                   c(0.55, 0.99, 0.01, 0.85)))

screen(1)
par(mar=c(0,0,0,0))
plot(NA, xaxt='n',yaxt='n',bty='n',ylab='',xlab='', xlim=c(0,10), ylim=c(0, 10))
mtext(paste0(fips$State, " (", GSOM_Longest$name, ")"), side=3, line=-1, cex=1.4)

screen(2)

# Determine xg (range)
dest <- densEstBayes(GSOM2$TMIN.anom, method = "NUTS"); dest$range.x

control = densEstBayes.control(range.x = dest$range.x, 
      numBins = 401,
      numBasis = 50, sigmabeta = 1e5, ssigma = 1000,
      convToler = 1e-5, maxIter = 500, nWarm = NULL,
      nKept = NULL, nThin = 1, msgCode = 1)

#destSMFVB <- densEstBayes(GSOM2$TMIN.anom, method = "SMFVB", control = control)
#plot(destSMFVB, plotIt=T, xlab = "TMIN", main = "", setCol=h.ramp[i])
par(las=1, mar=c(4, 4, 0, 0) + 0.1)
for(i in 1:length(unique(GSOM2$Score))){
   # i = 13
   GSOM2sub = GSOM2[GSOM2$Score==sort(unique(GSOM2$Score))[i],]
   dest <- densEstBayes(GSOM2sub$TMIN.anom, method = "NUTS", control = control)
   xg = plot(dest, plotIt=FALSE)$xg
   densEstg = plot(dest, plotIt=FALSE)$densEstg
   
   if(i==1) plot(0, type = "n", bty = "l", 
         xlim=range(xg), ylim=c(0,0.25), 
         xlab = "TMIN anomaly (°F)", main = "", ylab="Density")
   lines(xg, densEstg, col=h.ramp[i])
rug(jitter(GSOM2sub$TMIN.anom,amount = 0.2), col=h.ramp[i])
}

screen(3)
par(mar=c(0,0,1,0))
plot(NA,xaxt='n',yaxt='n',bty='n',ylab='',xlab='', xlim=c(0,10), ylim=c(0,10))
# 
legend("topright", inset=c(0,0), bg="transparent", bty="n",
       legend=unique(GSOM2$Score), 
       fill=h.ramp, horiz=FALSE, cex=0.85)

screen(4)
par(las=1, mar=c(4, 4, 0, 0) +0.1)
# Determine xg (range)
dest <- densEstBayes(GSOM2$TMAX.anom, method = "NUTS"); dest$range.x

control = densEstBayes.control(range.x = dest$range.x, 
      numBins = 401,
      numBasis = 50, sigmabeta = 1e5, ssigma = 1000,
      convToler = 1e-5, maxIter = 500, nWarm = NULL,
      nKept = NULL, nThin = 1, msgCode = 1)

for(i in 1:length(unique(GSOM2$Score))){
   # i = 13
   GSOM2sub = GSOM2[GSOM2$Score==sort(unique(GSOM2$Score))[i],]
   dest <- densEstBayes(GSOM2sub$TMAX.anom, method = "NUTS", control = control)
   xg = plot(dest, plotIt=FALSE)$xg
   densEstg = plot(dest, plotIt=FALSE)$densEstg
   
   if(i==1) plot(0, type = "n", bty = "l", 
         xlim=range(xg), ylim=c(0,.25), 
         xlab = "TMAX anomaly (°F)", main = "", ylab="Density")
   lines(xg, densEstg, col=h.ramp[i])
rug(jitter(GSOM2sub$TMIN.anom,amount = 0.2), col=h.ramp[i])
}
#rug(jitter(GSOM2sub$TMIN.anom,amount = 0.2), col=h.ramp[i])

close.screen(all.screens = TRUE)
dev.off()
} else {
   print("Skipping estimated density distribution chunk")}
@

The process to create these figures is very time consuming, so in general, I need to come up with an if then statement to avoid creating these everytime!

%\begin{figure}
%\includegraphics[width=1.00\textwidth]{\Sexpr{paste0(png_private, GSOM_estPDF.png)}}
%\caption{The changing in monthly temperature data.}
%\label{fig:GSOM_estPDF}
%\end{figure}

\section{Animated GIFs}

So far, this creates a gif file, but I haven't been able get the gif in the pdf directly yet. I will need an additional package or create separate png that are combined. For now, we'll create a gif file to be used in separate documents.

\subsection{Probability Distributions}

<<animated_normalPDFs, results='hide', eval=TRUE, echo=FALSE>>=

GSOM_dnorm.gif = paste0(fips$State2, "_", stid, "_GSOM_dnorm.gif")

if(!file.exists(paste0(gif_private, GSOM_dnorm.gif))){
   print("Creating animated normal probability")
   
# Define an image_graph size
img <- image_graph(600, 480, res = 96)

# START ------------------------------------------

ylim_new=NA
for(i in 1:length(unique(GSOM2$Decade))) 
   {
# i = 9
decade=(unique(GSOM2$Decade))[order(unique(GSOM2$Decade))==i]

GSOM2sub <- GSOM2[GSOM2$Decade==decade,]
h.ramp <- rev(heat.colors(length(unique(GSOM2$Decade))+1))[-1]
   
# Determine Stats for PDFs
TMAX.mean.anomaly.decade = aggregate(TMAX.anom ~ Decade, GSOM2sub, mean)
TMAX.sd.anomaly.decade = aggregate(TMAX.anom ~ Decade, GSOM2sub, sd)
names(TMAX.sd.anomaly.decade)=c("Decade", "TMAX.sd.anom")
TMIN.mean.anomaly.decade = aggregate(TMIN.anom ~ Decade, GSOM2sub, mean)
TMIN.sd.anomaly.decade = aggregate(TMIN.anom ~ Decade, GSOM2sub, sd)
names(TMIN.sd.anomaly.decade)=c("Decade", "TMIN.sd.anom")

TMAX.temp = merge(TMAX.mean.anomaly.decade, TMAX.sd.anomaly.decade, by="Decade")

TMIN.temp = merge(TMIN.mean.anomaly.decade, TMIN.sd.anomaly.decade, by="Decade")

GSOM.Monthly.Anom.mean.sd = merge(TMAX.temp, TMIN.temp, by="Decade")

par(las=1, mfrow=c(1,2), mar= c(4, 4, 2, 1) + 0.1)

Anom.x = seq(min(GSOM2$TMAX.anom), max(GSOM2$TMAX.anom), by=.1)
plot(Anom.x, dnorm(Anom.x, mean=GSOM.Monthly.Anom.mean.sd$TMAX.anom[1], 
   sd=GSOM.Monthly.Anom.mean.sd$TMAX.sd.anom[1]), ty="l", col=h.ramp[i], ylab="Density", xlab="TMAX Anomaly")
abline(v=mean(GSOM2$TMAX.anom[GSOM2$Decade==min(GSOM$Decade)]))
mtext(paste0(fips$State, " ", decade), side=3)

Anom.x = seq(min(GSOM2$TMIN.anom), max(GSOM2$TMIN.anom), by=.1)
plot(Anom.x, dnorm(Anom.x, mean=GSOM.Monthly.Anom.mean.sd$TMIN.anom[1], 
   sd=GSOM.Monthly.Anom.mean.sd$TMIN.sd.anom[1]), ty="l", col=h.ramp[i], ylab="Density", xlab="TMIN Anomaly")
abline(v=mean(GSOM2$TMIN.anom[GSOM2$Decade==min(GSOM$Decade)]))
mtext(paste0(fips$State, " ", decade), side=3)
}

par(las=1, mfrow=c(1,2), mar= c(4, 4, 2, 1) + 0.1)

TMAX.anomaly.decade = aggregate(TMAX.anom ~ Decade, GSOM2, 
   FUN = function(x) c(mean = mean(x), sd = sd(x)))
TMIN.anomaly.decade = aggregate(TMIN.anom ~ Decade, GSOM2, 
   FUN = function(x) c(mean = mean(x), sd = sd(x)))


Anom.x = seq(min(GSOM2$TMIN.anom), max(GSOM2$TMIN.anom), by=.1)
plot(Anom.x, dnorm(Anom.x, mean=TMIN.anomaly.decade$TMIN.anom[[1,1]], 
   sd=TMIN.anomaly.decade$TMIN.anom[[1,2]]), ty="l", col=h.ramp[1], ylab="Density", xlab="TMIN Anomaly")
mtext(paste0(fips$State, " ", decade), side=3)
for(i in 2:nrow(TMIN.anomaly.decade)){
lines(Anom.x, dnorm(Anom.x, mean=TMIN.anomaly.decade$TMIN.anom[[i,1]], sd=TMIN.anomaly.decade$TMIN.anom[[i,2]]), col=h.ramp[i])
}
abline(v=mean(GSOM2$TMIN.anom[GSOM2$Decade==min(GSOM$Decade)]), col="blue")
abline(v=mean(GSOM2$TMIN.anom[GSOM2$Decade==max(GSOM$Decade)]), col="red")

Anom.x = seq(min(GSOM2$TMAX.anom), max(GSOM2$TMAX.anom), by=.1)
plot(Anom.x, dnorm(Anom.x, mean=TMAX.anomaly.decade$TMAX.anom[[1,1]], 
   sd=TMAX.anomaly.decade$TMAX.anom[[1,2]]), ty="l", col=h.ramp[1], ylab="Density", xlab="TMAX Anomaly")
mtext(paste0(fips$State, " ", decade), side=3)
for(i in 2:nrow(TMAX.anomaly.decade)){
lines(Anom.x, dnorm(Anom.x, mean=TMAX.anomaly.decade$TMAX.anom[[i,1]], sd=TMAX.anomaly.decade$TMAX.anom[[i,2]]), col=h.ramp[i])
}
abline(v=mean(GSOM2$TMAX.anom[GSOM2$Decade==min(GSOM$Decade)]), col="blue")
abline(v=mean(GSOM2$TMAX.anom[GSOM2$Decade==max(GSOM$Decade)]), col="red")


# END -----------------------------------------------------
dev.off()

GSOM_animation <- image_animate(img, fps = 1, loop=2, optimize = TRUE)
#print(GSOM_animation)

image_write(GSOM_animation, paste0(gif_private, GSOM_dnorm.gif))

} else {
   print("Skipping animated normal distribution chunk")}
@


%The file is saved in the main directory. 


\subsection{4 Weather Trend Plots}

<<animate, results='hide', eval=TRUE>>=
panel4.gif = paste0(fips$State2, "_", stid, "_4panel.gif")

if(!file.exists(paste0(gif_private, panel4.gif))){
   print("Creating animated 4panel.gif")  

img <- image_graph(600, 480, res = 96)
# START ----
ylim_new=NA
for(i in seq(min(GSOM$Year), max(GSOM$Year), by=2)) 
   {
par(las=1, mfrow=c(4,1), mar= c(2, 4, 2, 1) + 0.1)
# TMINmonthMax
   GSOMsub <- GSOM[GSOM$Month==TMINmonthMax & GSOM$Year<=i,]
   if(nrow(GSOMsub)<10) next
plot(TMIN~Date, GSOMsub[GSOMsub$Month==TMINmonthMax,], 
   col='gray70', pch=20, xlab="", 
   main=paste("Mean", format(GSOMsub$Date,"%B")[1], 
              "Min. Temp", GSOM_Longest$name))
GSOM.lm = lm(TMIN~Date, GSOMsub)
pred_dates <-data.frame(Date = GSOMsub$Date); 
nrow(pred_dates); pred_dates
#Predits the values with confidence interval 
ci <- predict(GSOM.lm, newdata = pred_dates, 
              interval = 'confidence')
lines(pred_dates$Date, as.numeric(ci[,1]), col="darkred")
lines(pred_dates$Date, as.numeric(ci[,2]), col="darkorange")
lines(pred_dates$Date, ci[,3], col="darkorange")
location_index = round(length(GSOMsub$Date) * 0.99,0)
text(pred_dates$Date[location_index], ci[location_index,3], 
     paste(report_prob2(GSOM.lm)), pos=2, cex=1.5)

# Box Plot of TMAX by Month
CHCNDsub = subset(CHCND, CHCND$Year<=i, 
      select=c(Month, Month.name, TMAX, TMIN))
boxplot(TMAX ~ Month.name, data=CHCNDsub, xlab="", main="")
symbol.y = (par()$yaxp[2])-diff(par()$yaxp[1:2])*.99
#symbol.y = (par()$yaxp[2])
text(sumstats$Month, symbol.y, sumstats$TMAX_Symbol, 
     col="red", cex=2)
mtext(paste("Maximum Daily Temperatures", min(CHCND$Year), 
      "-", i, GSOM_Longest$name), line=1)
mtext("(NOTE: Red astrisks correspond to signficant changes)", 
      line=0, cex=.7)

# TMAXmonthMax 
GSOMsub <- GSOM[GSOM$Month==TMAXmonthMax & GSOM$Year<=i,]
ylim = range(GSOMsub$TMAX)
#if(!is.na(ylim_new)) ylim[2]=ylim_new
plot(TMAX~Date, GSOMsub, col='gray70', pch=20,
     ylim=ylim, xlab="",
     main=paste("Mean", format(GSOMsub$Date,"%B")[1], 
                "Max. Temp", GSOM_Longest$name))
GSOM.lm = lm(TMAX~Date, GSOMsub) 

ci <- predict(GSOM.lm, newdata = pred_dates, 
              interval = 'confidence')
lines(pred_dates$Date, as.numeric(ci[,1]), col="darkred")
lines(pred_dates$Date, as.numeric(ci[,2]), col="darkorange")
lines(pred_dates$Date, ci[,3], col="darkorange")

text(pred_dates$Date[location_index], ci[location_index,3], 
     paste(report_prob2(GSOM.lm)), pos=2, cex=1.5)

# Record High Temperatures
# START
   j = which(years %in% i)  
   if(sum(is.na(TMAX.mat.noleap[,j]))==366) next
TMAX1 = apply(TMAX.mat.noleap[,1:j], 1, function (x) which.max(x)); 
is.na(TMAX1) <- lengths(TMAX1) == 0
TMAX1 <- unlist(TMAX1)
TMAX1 <- count(TMAX1)
#str(TMAX1)
names(TMAX1)=c("Year", "TMAX")
TMAX_na = data.frame(Year=1:j)
TMAX <- merge(TMAX_na, TMAX1, all.x=TRUE, by="Year")

if(sum(is.na(TMIN.mat[,j]))==366) next
   # Select Minimum and Change to Negative Value
TMIN1 = apply(TMIN.mat[,1:j], 1, function (x) which.min(x)); 
is.na(TMIN1) <- lengths(TMIN1) == 0
TMIN1 <- unlist(TMIN1)
TMIN1 <- count(TMIN1) # Max Counts Negagive
#str(TMIN1)   
names(TMIN1)=c("Year", "TMIN")
TMIN_na <- data.frame(Year=1:j)
TMIN <- merge(TMIN_na, TMIN1, all.x=TRUE, by="Year")

R1 <- merge(TMAX, TMIN, by="Year")
R1$Index = rep(j, nrow(R1))
#results = rbind(results, R)
R1$TMIN = -R1$TMIN
## Sorting out X Axis
tic.no <- 4
rowskip = round(nrow(R1)/tic.no, 0)
row_numb <- seq_len(nrow(R1)) %% rowskip
row.sel = which(row_numb %in% c(1))
index.year <- years[row.sel]
# switch to decades?

xtics = row.sel
xlabs = index.year

yrange = range(c(R1$TMIN, R1$TMAX), na.rm=T)
ytics = floor(seq(yrange[1], yrange[2], length.out=tic.no))
ylabs = as.character(abs(ytics))

par(las=1, xpd=TRUE)
plot(c(1,nrow(R1)), c(yrange[1], yrange[2]), ty="n", xaxt='n', yaxt='n', ylab="No. of Record Temps", xlab="", main="Record Highs and Lows")
axis(2,at=ytics, labels=ylabs)
axis(1,at=xtics, labels=xlabs)
barplot(height = R1$TMAX, space=0, add = TRUE, axes = FALSE, col="red")
barplot(height = R1$TMIN, space=0, add = TRUE, axes = FALSE, col="blue")
# END
}

# STOP ----
dev.off()

GSOM_animation <- image_animate(img, fps = 1, loop=2, optimize = TRUE)
image_write(GSOM_animation, paste0(gif_private, panel4.gif)) 

} else {
   print("Skipping animated GSOM_4plots chunk")}

@


The file is saved in the main directory. 


\subsection{Evaluating Records}

TBD

\subsection{Export Options}

TBD

\section{Sea Surface Temperature Data -- SURP PROJECT WAITING TO HAPPEN}

In contrast to terrestrial data, sea surface temperature (SST) is quite difficult to obtain and process. There are numerous tools to access the data, but they often require knowledge of complex software tools that are not easy to set up or programming experience with python or others.

\url{https://climexp.knmi.nl/select.cgi?id=someone@somewhere&field=ersstv5}

There are, however, a few tools build for R users that seem to accomplish all that we need. 

\url{https://rda.ucar.edu/index.html?hash=data_user&action=register}

\url{https://rda.ucar.edu/datasets/ds277.9/}

Alternatively, we can download flat ascII tables of gridded data:

\url{https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/}


<<echo=FALSE, eval=FALSE>>=

library(chron)
library(RColorBrewer)
library(lattice)
#library(ncdf)
library(ncdf4)
#library(greenbrown) # for gridded trend analysis

ersst.nc = "/home/CAMPUS/mwl04747/github/Climate_Change_Narratives/Data/FA19/ersst.v5.185401.nc"
Y1854 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1854.asc"
Y1864 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1864.asc"
Y1874 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1874.asc"
Y1884 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1884.asc"
Y1894 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1894.asc"
Y1904 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1904.asc"
Y1914 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1914.asc"
Y1924 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1924.asc"
Y1934 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1934.asc"
Y1944 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1944.asc"
Y1954 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1954.asc"
Y1964 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1964.asc"
Y1974 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1974.asc"
Y1984 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1984.asc"
Y1994 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.1994.asc"
Y2004 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.2004.asc"
Y2014 = "https://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/ascii/ersst.v5.2014.asc"

temp = rbind(read.table(Y1854)[75,67], read.table(Y1864)[75,67], read.table(Y1874)[75,67],
read.table(Y1884)[75,67], read.table(Y1894)[75,67], read.table(Y1904)[75,67],
read.table(Y1914)[75,67], read.table(Y1924)[75,67], read.table(Y1934)[75,67],
read.table(Y1944)[75,67], read.table(Y1954)[75,67], read.table(Y1964)[75,67],
read.table(Y1974)[75,67], read.table(Y1984)[75,67], read.table(Y1994)[75,67],
read.table(Y2004)[75,67], read.table(Y2014)[75,67])

temp.df = data.frame(Temp = as.vector(temp)/100); temp.df
temp.df$Year = seq(1854, 2014, 10)
plot(Temp~ Year, temp.df)
abline(coef(lm(Temp~Year, data=temp.df)), col="red")
#automating this process!

directory = "/pub/data/cmb/ersst/v5/ascii"

B195401 = nc_open(ersst.nc)


# str(B195401)
# print(B195401)

ncin = B195401

print(ncin)
lon <- ncvar_get(ncin, "lon")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin, "lat", verbose = F)
nlat <- dim(lat)
head(lat)

print(c(nlon, nlat))

t <- ncvar_get(ncin, "time")
tunits <- ncatt_get(ncin, "time", "units")
nt <- dim(t); nt

lat.sel = 67; lon.set = 75

#ncvar_get(ncin, sst) #object 'sst' not found

#ncvar_get(ncin, var$sst) object of type 'closure' is not subsettable
#ncvar_get(ncin, var) second argument to ncvar_get must be an object of type ncvar or ncdim (both parts of the ncdf object returned by nc_open()), the character-string name of a variable or dimension or NA to get the default variable from the file.  If the file is netcdf version 4 format and uses groups, then the fully qualified var name must be given, for example, model1/run5/Temperature

ncvar_get(ncin, "sst") #spits out the temperatures. but why the negative numbers!

# tmp.array <- ncvar_get(ncin, dname) # doesn't work...

tmp.array <- ncvar_get(ncin, "sst")
dim(tmp.array)

tmp.array[75, 67]

tmp.array[67,]

dlname <- ncatt_get(ncin, "sst", "long_name")
dunits <- ncatt_get(ncin, "sst", "units")
fillvalue <- ncatt_get(ncin, "sst", "_FillValue")
dim(tmp.array)

title <- ncatt_get(ncin, 0, "title")
institution <- ncatt_get(ncin, 0, "institution")
datasource <- ncatt_get(ncin, 0, "source")
references <- ncatt_get(ncin, 0, "references")
history <- ncatt_get(ncin, 0, "history")
Conventions <- ncatt_get(ncin, 0, "Conventions")

# split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth = as.integer(unlist(tdstr)[2])
tday = as.integer(unlist(tdstr)[3])
tyear = as.integer(unlist(tdstr)[1])
chron(t, origin = c(tmonth, tday, tyear))

# tmp.array[tmp.array == fillvalue$value] <- NA

# length(na.omit(as.vector(tmp.array[, , 1])))

m <- 1
tmp.slice <- tmp.array[, , m]

image(lon, lat, tmp.array, col = rev(brewer.pal(10, "RdBu")))

# image(lon, lat, tmp.slice, col = rev(brewer.pal(10, "RdBu")))


@

\section{Satellite Data}

TBD

\section{Ice-Core Data}

TBD

\section{Conclusions}

Developing a robust method to analyze weather stations is both time consuming and difficult to justify the outcome. In part because the data suggest that each station (region) requires different types of analysis, based on the expected patterns of temperature and rainfall. As climate scientists have known for decades, the terminology of global warming is not very useful. Not because scientists are trying to hide something or promote some biased agenda, but that even as warming of the global average is well documented, the impacts of climate change on each region is highly specific, requiring specificity in the analysis. 

Hopefully, this little analysis has created some mechanism for others to appreciate this compexity. 

<<endtime, echo=FALSE, warning=FALSE>>=

# Create CSV with filenames for blog/
paste0(GSOM_Longest$name, " (ID: ", GSOM_Longest$id, ")")

dbase = data.frame(State = fips$State, 
   Station_ID = paste0(GSOM_Longest$name, " (ID: ", GSOM_Longest$id, ")"), 
   Startyear = startyear, Endyear = endyear, 
   gif_private = gif_private, png_private = png_private, 
   Map.png = map.png, 
   GSOM_1975.png = GSOM_1975.png, 
   GSOM_1975_anomaly.png = GSOM_anomaly_1975.png,
   Records.png = records.png, 
   panel4.png = panel4.png,
   GSOM_dnorm.png = GSOM_dnorm.png,
   KISS.png = KISS.png,
   GSOM_estPDF.png = GSOM_estPDF.png,
   panel4.gif = panel4.gif,
   GSOM_dnorm.gif = GSOM_dnorm.gif
)

# print(dbase)
write.table(dbase, file = "/home/CAMPUS/mwl04747/github/Climate_Change_Narratives/Social_Media/State_htmls/dbase.csv", 
            append = TRUE, quote = TRUE, sep = ",",
            eol = "\n", na = "NA", dec = ".", row.names = FALSE,
            col.names = FALSE, #qmethod = c("escape", "double"),
            )

end_time <- Sys.time()
(totaltime = end_time - start_time)
rm(dbase)
@

%The document took \Sexpr{round(totaltime,1)} minutes to process and compile. My next goal will be to optimize the process and streamline the time to analyze. 

% bibilography section here-------------------------------------------
%\clearpage

\bibliographystyle{apalike}
%\renewcommand\bibname{References}{}
\bibliography{/home/mwl04747/RTricks/references}%	\addcontentsline{toc}{chapter}{References}

\end{document}
