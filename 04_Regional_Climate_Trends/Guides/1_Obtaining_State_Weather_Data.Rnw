\documentclass{article}

\title{Obtaining and Cleaning NOAA Weather Station Data}
\author{Marc Los Huertos}
\date{\today~(ver. 0.75)} 

\begin{document}
\maketitle

\section{Introduction}

\subsection{Goals}

Using a list of active weather stations in the United States, you  will download and process the data to create a time series of temperature anomalies. \section{Evaluating Terrestrial Meteorological Data}

\subsection{Selected History of Climate Science}

Geologists have known the climate has been changing over the Earth's history. But what causes these changes has been a major research area for over 100 years. There are numerous drivers that contribute to changing climates -- including the arrangement of the continents on the planet, the distance to the sun, energy generated by the sun, volanic activity, and the composition of the Earth's atmosphere. 

It's the last one that we'll spend time because the Earth's temperature are changing pretty dramatically over the last 100 years and the cause is no mystery -- the human activity that has released carbon dioxide (CO$_2$) into the atmosphere. The two main sources of CO$_2$ is from land use change, e.g. deforestration, and the burning of fossil fuels, e.g. coal, oil, and natural gas. 

The first to propose the role of CO$_2$ on the Earth's atmosphere was a Swedish scientist Svante Arrhenius, who figured out that CO$_2$ absorbs infarred light. Moreover, he deduced that the Earth's temperature was actually warmer than it might otherwise be if CO$_2$ was not part of the Earth's atmoshere. 

\subsection{Why Look at Individual Stations?}

I don't think there is a single, perfect way to analyze and communicate climate change. But the beauty of the network of stations in the USA and around the world is that these stations record weather as expecienced by local people. And while indiviudual stations may not represent the overall regional and global patterns well, this give us a mechanism to connect local experiences to regional or global processes. 

Of course, some may fixate on the local pattern and remain unconvinced of the larger context and for those folks, there may be better ways to communicated climate data. 

However, I would be remiss in failing to mention that some may fixate on local patterns and use these patterns to ignore or to dimiss the patterns in other regions. 

Finally, the impacts of climate change are highly specific to the region in question. Thus, once someone understands the impacts on climate change in their region, they my not be able to appreciate how differnet the climate impacts might affect other peoples, who maybe more vulneratble, around the globe. 

Thus, with these weaknesses in mind, I will pursue this project with an eye to address these other issues at later stages.

\subsection{Approach}

\subsubsection{NOAA Data Records}

The US National Oceanic and Atmospheric Administration (NOAA) maintains several sources of digital weather data from the USA and beyond. These data have been collected from stations around the country to support a wide range of human activities that include farming, aviation, shipping, and even armed conflict. 

At various times, these records have been used to evaluate long-term climate change with varying success. Without a doubt, these data are not perfect, but they remain that foundation of an effective adn professionally maintained environmental monitoring program that engenders integrity, even when facing budget cuts. 

I will use these data to select for a station with a long record for each state in the USA. Future projects might evaluate the record for stations around the world, but we will see about that. 

\subsubsection{R Programming Language}

R is an open source programming environment that has become one of the most popular tools for statiticians and data scientists. Capitalizing on the open source framework, a wide range of libraries or packages have been developed to faciliate data processing, analysis, and graphical displays. 

\subsection{Selecting Weather Records by State}

There are numerous ways to analyze temperature records, where stations can be analyzed individually or records could be sampled and analyzed in spatially in grids. Each of these are valid approaches depending on the question to be addressed. 

Here are the questions we will address: 

\begin{itemize}
  \item What stations have the longest meterological records in the USA?
  \item Can we determine the reliability of these stations?
  \item Finally, is there a temperature trend?

\subsubsection{Identify List of State IDs (FIPS)}

Using the rNOAA library in R, we can query NOAA's database to identify station codes (FIPS) by state. With the states and some territories, there are 55 FIPS for US weather stations. Althogh these include the District of Columbia, they do not include US Territories, sich as Guam, Puerto Rico, Marshall Islands, etc. We'll have to use a different search code for these.

rNOAA has a simple function to list for each of the states and the weather stations in each. I use ncdc\_locs() functions to select each state and ncdc\_station() to obtain the station ids with the longest records. 

The function queries the NOAA website and retrieves state codes, ``FIPS:XX''. Each state has a number of weather stations,\footnote{Project Idea: It would be nice to make a map of how concentrated the stations spatially.} some with a long record, some with a short record, and some with numerous interruptions. Our goal is to select a long record with few missing data. 

\subsubsection{Selection Stations}

With the state ids, we can evaluate the metadata for all the weather stations, which will work to get the longest records, using \texttt{ncdc\_stations()}. 

First, we subset the data for stations that actively collecting data. Then we'll sort to the active stations to find the one with the longest records. We will use these stations for our analysis. 

There were some records that didn't have robust TMAX/TMIN records, so there are some states that I had to manually select an alternative stations. 

\subsection{Read Data}

First, we install some packages and read in the data. I suggest you create a folder for the project (I created one called "04\_Regional\_Climate\_Trends") and then used the function here() to get the working directory and read the csv into R. This might be easier than the file.choose() option, but you can use that if you prefer.

<<>>=
library(here)
library(xtable)

stations.active.oldest = read.csv(
  here("04_Regional_Climate_Trends", "stations.active.oldest.csv"))

# OR
# use file.choose() to select the file
# filename = "MY.PATH/04_Regional_Climate_Trends/stations.active.oldest.csv"
# stations.active.oldest = read.csv(filename)
@ 


\subsection{Map US Weather Stations}

Here's a map of the weather stations in the dataset. Pretty lame map!  We'll make a better one later.\footnote{Evelyn/Brody: This is a good change to see how to use R for map making! First we need to transform that data.}

<<>>=
plot(stations.active.oldest$LONGITUDE, 
     stations.active.oldest$LATITUDE, 
     xlab = "Longitude",ylab = "Latitude", 
     pch=20, cex=0.3, col='gray60', las=1,
     main = "US Weather Stations")
@
\subsection{Select and Evaluate State Data}

<<>>=
stations.unique = 
  unique(stations.active.oldest[,c("STATE", "STATE_NAME")])

xtab = xtable(stations.unique)
@

The each of you will select a state -- see the Google Sheet sign up so we have a diverse set of states.

<<>>=
my.state = "CA" # change the "CA" to your state
@

\section{Download Data from NOAA}

\subsection{Subset Station Data by State}

This uses the stations.active.oldest file to download the data from the NOAA website based on the state you have choose.

<<>>=# Select Stations in State
my.stations = subset(stations.active.oldest, STATE == my.state)

# Download Updated Station Data
i=1
here::here("04_Regional_Climate_Trends", my.stations$ID[i])
@


The loop is not ideal -- loops are slow in R and prone to break -- but it should work for our purposes.\footnote{Evelyn/Brody: I wonder if we can test if the file exists so we don't have to downwload every time!} 
<<>>=
for(i in 1:nrow(my.stations)){
  url = paste0("https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/", 
               my.stations$ID[i], 
               ".csv.gz")
  
print(i) # Print Index Number
download.file(url, paste0(here::here("04_Regional_Climate_Trends", 
                                     "Data", 
                                     "SP24/"), 
                          my.stations$ID[i], 
                          ".csv.gz"), 
              quiet = FALSE, mode = "w", cacheOK = TRUE)

assign(paste0("station", i), 
       read.csv(gzfile(paste0(here::here("04_Regional_Climate_Trends", 
            "Data", "SP24/"),my.stations$ID[i], ".csv.gz")), 
            header=FALSE))

# can't get the header named in loop! Grrr...
#names(paste0("station",i)) <- c("ID", "DATE", "ELEMENT", 
# "VALUE", "M-FLAG", "Q-FLAG", "S-FLAG", "OBS-TIME")

} # LOOP END

# Fix Variable Names based on NOAA Documentation
names(station1) <- c("ID", "DATE", "ELEMENT", "VALUE", 
                     "M-FLAG", "Q-FLAG", "S-FLAG", "OBS-TIME")
names(station3) <- names(station2) <- names(station1)
names(station5) <- names(station4) <- names(station1)

# NAMES OF VARIABLES ARE INCORRECT for some STATIONS??
      
  #ID = 11 character station identification code
  #YEAR/MONTH/DAY = 8 character date in YYYYMMDD format 
  #                     (e.g. 19860529 = May 29, 1986)
  #ELEMENT = 4 character indicator of element type 
  #DATA VALUE = 5 character data value for ELEMENT 
  #M-FLAG = 1 character Measurement Flag 
  #Q-FLAG = 1 character Quality Flag 
  #S-FLAG = 1 character Source Flag 
  #OBS-TIME = 4-character time of observation in hour-minute format
  #                     (i.e. 0700 =7:00 am); if no ob time information 
 #is available, the field is left empty

@

\section{Process and Clean Data}


I have created a "function" that can process and clean the data, if the data are consistent!  If not, we'll trouble shoot together. 

Here's the data structure, using str(), but if you have something different, please let me know and we'll sort out how to fix it. 

<<>>=
str(station1)
@


\subsection{Clean Data}

First, I tested each line on station1. I will then create a function to clean the data and apply it to each station.

<<>>=
# station1$VALUE = station1$VALUE/10  # Correct Values Units
# Fix Date format
station1$Ymd = as.Date(as.character(station1$DATE), format = "%Y%m%d") 
str(station1)

station1$MONTH = as.numeric(format(station1$Ymd, "%m"))
station1$YEAR = as.numeric(format(station1$Ymd, "%Y"))
station1.monthly = aggregate(VALUE ~ MONTH + YEAR, 
                   data = subset(station1, ELEMENT == "TMAX"), mean)

# create baseline (normals) dataset
station1.normals = subset(station1, 
                           Ymd >= "1961-01-01" & Ymd <= "1990-12-31") 
station1.normals.monthly = aggregate(VALUE ~ MONTH, 
                   data = subset(station1.normals, ELEMENT == "TMAX"), mean)
names(station1.normals.monthly) <- c("MONTH", "NORMALS")

station1.anomaly = merge(station1.monthly, 
                         station1.normals.monthly, by = "MONTH")
station1.anomaly$ANOMALY = 
  station1.anomaly$VALUE - station1.anomaly$NORMALS

@

\subsection{Clean Data Function}

Function is probably sensitive to missing values, need to work on that!

<<cleandataframe.fun, echo=TRUE>>=
x=station1
cleandataframe.fun <- function(x){
  #x$VALUE = x$VALUE/10
  x$Ymd = as.Date(as.character(x$DATE), format = "%Y%m%d")
  x$MONTH = as.numeric(format(x$Ymd, "%m"))
  x$YEAR = as.numeric(format(x$Ymd, "%Y"))

  x.TMAX.monthly = aggregate(VALUE ~ MONTH + YEAR, 
            data = subset(x, ELEMENT == "TMAX"), mean)
  names(x.TMAX.monthly) <- c("MONTH", "YEAR", "TMAX")
  x.TMIN.monthly = aggregate(VALUE ~ MONTH + YEAR, 
            data = subset(x, ELEMENT == "TMIN"), mean)
  names(x.TMIN.monthly) <- c("MONTH", "YEAR", "TMIN")
  x.PRCP.monthly = aggregate(VALUE ~ MONTH + YEAR, 
            data = subset(x, ELEMENT == "PRCP"), sum)
  names(x.PRCP.monthly) <- c("MONTH", "YEAR", "PRCP")
  
  x.normals = subset(x, Ymd >= "1961-01-01" & Ymd <= "1990-12-31")  
  x.TMAX.normals.monthly = aggregate(VALUE ~ MONTH, 
            data = subset(x.normals, ELEMENT == "TMAX"), mean)
  names(x.TMAX.normals.monthly) <- c("MONTH", "NORMALS")
  x.TMIN.normals.monthly = aggregate(VALUE ~ MONTH, 
            data = subset(x.normals, ELEMENT == "TMIN"), mean)
  names(x.TMIN.normals.monthly) <- c("MONTH", "NORMALS")
  x.PRCP.normals.monthly = aggregate(VALUE ~ MONTH, 
            data = subset(x.normals, ELEMENT == "PRCP"), sum)
  names(x.PRCP.normals.monthly) <- c("MONTH", "NORMALS")
  
  
  x.TMAX.anomaly = merge(x.TMAX.monthly, x.TMAX.normals.monthly, by = "MONTH")
  x.TMAX.anomaly$TMAX.anomaly = x.TMAX.anomaly$TMAX - x.TMAX.anomaly$NORMALS
  
  x.TMIN.anomaly = merge(x.TMIN.monthly, x.TMIN.normals.monthly, by = "MONTH")
  x.TMIN.anomaly$TMIN.anomaly = x.TMIN.anomaly$TMIN - x.TMIN.anomaly$NORMALS
  
  x.PRCP.anomaly = merge(x.PRCP.monthly, x.PRCP.normals.monthly, by = "MONTH")
  x.PRCP.anomaly$PRCP.anomaly = x.PRCP.anomaly$PRCP - x.PRCP.anomaly$NORMALS
  
  TEMP <- merge(x.TMAX.anomaly, x.TMIN.anomaly, by = c("MONTH", "YEAR") )
  x.anomaly <- merge(TEMP, x.PRCP.anomaly, by = c("MONTH", "YEAR"))[,c(1:3, 5:6, 8:9, 11)]
  library(lubridate)
  #x.anomaly$Ym1 = as.Date(paste(x.anomaly$YEAR, x.anomaly$MONTH), format="%Y %m")
  x.anomaly$Ym1 =  lubridate::myd(paste(x.anomaly$MONTH, x.anomaly$YEAR, "1"))
  str(x.anomaly)
  return(x.anomaly)
}
@

\subsection{Apply Function to All Stations}

So far, I have only run function for 1 station, but I suspect you can figure out how to run it for each one!

<<>>=
station1.clean= cleandataframe.fun(station1)
@

\subsection{Plot Anomaly}

Graphic has lots of issues. more next time!  But here's a start.

<<>>=
options(scipen=5)
par(mar=c(4,6,2,5))

plot(ANOMALY ~ YEAR, data = subset(station1.TMAX, MONTH == 1), 
     las=1, pch=19, col = "blue", cex=.5, #xlab = "Year", 
     ylab = "Maximum Temp Anomaly (C)",
     main="January Maximum Temp Anomaly")
mtext("Maximum Temp Anomaly (C)", side = 2, line = 3)
temp.lm = lm(ANOMALY ~ YEAR, data = subset(station1.TMAX, MONTH == 1))
abline(coef(temp.lm), col = "red")

@

\seciton{QA/QC}

\subsection{Missing Data}

<<>>=
# determine percent missing in station1
station1.TMAX.coverage = sum(!is.na(station1$VALUE[station1$ELEMENT=="TMAX"]))/length(station1$VALUE[station1$ELEMENT=="TMAX"])*100

# function to determine percent missing
coverage.fun <- function(station, element){
  Dates.all = data.frame(Ymd=seq.Date(from=min(station$Ymd), to=max(station$Ymd), by="day"))
  station.full = merge(Dates.all, station, all = TRUE)
  station.coverage = sum(!is.na(station.full$VALUE[station.full$ELEMENT==element]))/
    length(station.full$VALUE[station.full$ELEMENT==element])*100
  return(round(station.coverage,2))
}

coverage.data(station1, "TMAX")
coverage.data(station2, "TMAX")

Date.full = data.frame(Ymd=seq.Date(from=min(station1$Ymd), to=max(station1$Ymd), by="day"))
str(Date.full)

station1.full = merge(Date.full, station1, all = TRUE)
coverage.fun(station1, "TMAX")
coverage.fun(station2, "TMAX")


@
\section{Next Steps}

This is all we need to do so far. Next week, we'll look at different way to visualize the data! 

I'll save all the station data into csv files, then use them in the next guide to clean, process, and visualize data.

<<>>=
write.csv(station1, file = paste0(here::here("04_Regional_Climate_Trends", "Data", "SP24", "station1.csv")), row.names = FALSE)
write.csv(station2, file = paste0(here::here("04_Regional_Climate_Trends", "Data", "SP24", "station2.csv")), row.names = FALSE)
write.csv(station3, file = paste0(here::here("04_Regional_Climate_Trends", "Data", "SP24", "station3.csv")), row.names = FALSE)
write.csv(station4, file = paste0(here::here("04_Regional_Climate_Trends", "Data", "SP24", "station4.csv")), row.names = FALSE)
write.csv(station5, file = paste0(here::here("04_Regional_Climate_Trends", "Data", "SP24", "station5.csv")), row.names = FALSE)
@

\end{document}
