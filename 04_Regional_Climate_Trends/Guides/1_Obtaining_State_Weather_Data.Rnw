\documentclass{article}
\usepackage{natbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    hyperindex=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=purple,
    citecolor=violet
}

\title{Guide 1: Obtaining State Weather Station Data}
\author{Marc Los Huertos}
\date{\today~(ver. 0.89)} 

\begin{document}
\maketitle

\section{Introduction}

\subsection{Goals}

Using a list of active weather stations in the United States, you  will download the data from five stations and write weather station data csv files. 

%\section{Evaluating Terrestrial Meteorological Data}

\subsection{Selected History of Climate Science}

Geologists have known the climate has been changing over the Earth's history. But what causes these changes has been a major research area for over 100 years. There are numerous drivers that contribute to changing climates -- including the arrangement of the continents on the planet, the distance to the sun, energy generated by the sun, volanic activity, and the composition of the Earth's atmosphere. 

It's the last one that we'll spend time because the Earth's temperature are changing pretty dramatically over the last 100 years and the cause is no mystery -- the human activity that has released carbon dioxide (CO$_2$) into the atmosphere. The two main sources of CO$_2$ is from land use change, e.g. deforestration, and the burning of fossil fuels, e.g. coal, oil, and natural gas. 

The first person to propose the role of CO$_2$ on the Earth's atmosphere was a Swedish scientist Svante Arrhenius, who figured out that CO$_2$ absorbs infarred light \citep{rodhe1997svante}. Moreover, he deduced that the Earth's temperature was actually warmer than it might otherwise be if CO$_2$ was not part of the Earth's atmosphere. 

\subsection{Why Look at Individual Stations?}

I don't think there is a single, perfect way to analyze and communicate climate change. But the beauty of the network of stations in the USA and around the world is that these stations record weather as expecienced by local people. And while indiviudual stations may not represent the overall regional and global patterns well, this give us a mechanism to connect local experiences to regional or global processes. 

Of course, some may fixate on the local pattern and remain unconvinced of the larger context and for those folks, there may be better ways to communicated climate data. 

However, I would be remiss in failing to mention that some may fixate on local patterns and use these patterns to ignore or to dimiss the patterns in other regions. 

Finally, the impacts of climate change are highly specific to the region in question. Thus, once someone understands the impacts on climate change in their region, they my not be able to appreciate how differnet the climate impacts might affect other peoples, who maybe more vulneratble, around the globe. 

Thus, with these weaknesses in mind, I will pursue this project with an eye to address these other issues at later stages.

\subsection{Approach}

\subsubsection{NOAA Data Records}

The US National Oceanic and Atmospheric Administration (NOAA) maintains several sources of digital weather data from the USA and beyond. These data have been collected from stations around the country to support a wide range of human activities that include farming, aviation, shipping, and even armed conflict. 

At various times, these records have been used to evaluate long-term climate change with varying success. Without a doubt, these data are not perfect, but they remain that foundation of an effective adn professionally maintained environmental monitoring program that engenders integrity, even when facing budget cuts. 

I will use these data to select for a station with a long record for each state in the USA. Future projects might evaluate the record for stations around the world, but we will see about that. 

\subsubsection{R Programming Language}

R is an open source programming environment that has become one of the most popular tools for statiticians and data scientists. Capitalizing on the open source framework, a wide range of libraries or packages have been developed to facilitate data processing, analysis, and graphical displays. 

%For the project we'll use a few packages that need to be installed and loaded. Using the lower right panel, you can install the packages by clicking on the "Packages" tab and then "Install" and then type in the package name and click "Install".

\subsection{Revised Document Structure}

Based on some feedback in the last week, I have re-orgnaized the document with bullets of the steps and then the code. In the following sections, I'll descriptions about ``what the code does and why". But several folks see the text and get tuned out.

There are two weaknesses with this approach that I can see. First, if the code does not work, you will have a really hard time figuring out why, since you have no idea what the code is supposed to do. Second, if the code works, you will not really be learning any R coding. Just how to copy and paste.

I have created a bunch of functions, which can then be used to collect Let's try this and see how it works! Please download and run the code called Guide1.R, which has the functions for this handout.

<<setup, echo=FALSE, results='hide', message=FALSE>>=
library(here)
source(here("04_Regional_Climate_Trends", "Guides", "Guide1.R"))
@

\section{R Functions for Getting Weather Station Data}

\subsection{R Code with Custom Functions}

From the Canvas page, go to the Guide1functions.R file and download the file to your computer. Then uplaod the file to a directory in Rstudio. I suggest you create a new directory called ``Regional\_Climate\_Trends'' and then upload the file to that directory.

Open the file in Rstudio and run the code, using the ``source''. button near the top of the editor window.

Run the \href{https://github.com/marclos/RTricks/blob/master/04_Regional_Climate_Trends/Guides/Guide1.R}{Guide1functions.R} code and the functions will be loaded into your environment automatically.

\subsection{Function Descriptions and Use}

The following functions are used to select the weather stations, download data, and write weather station data csv files.

\begin{description}

\item[Subset Inventory Data with my.state]  The inventory data is a list of all the weather stations in the USA, US Territories, and Canada. This function will subset the data to only include the stations in the state of interest.

This function requires two parameters to be set: filename and my.state. 

Here's the function:

<<show_readInventory.fun, echo=FALSE>>=
print(readInventory.fun)
@

I suggest you use file.choose() and assign filename.csv to the path and name of the csv file. 

Example of how to use the function:

<<echo=TRUE, eval=TRUE>>=
my.state <- "CA"
filename.csv <- "/home/mwl04747/RTricks/04_Regional_Climate_Trends/stations.active.oldest.csv"
my.inventory <- readInventory.fun(filename.csv, my.state)
@

Creates a dataframe called my.inventory that contains the inventory data for the state/territory of interest.

\item[Download and Read Selected Weather Station Data into R]  This function will download the weather station data from the NOAA website and read it into R.

This functions requires two parameters: data path and name of inventory dataframe (my.inventory). The data path is a folder that you can always get your station data from if you need it. For example, if you need to update the data, you can just download the data from the NOAA website and then read it into R. 

<<show_downloadStation.fun, echo=FALSE>>=
print(downloadStations.fun)
@

The path to your data folder will be different, I created a directory to separate my data by class year, you don't need that! But it is nice to keep the data in a separate folder so things don't get to hectic.

An example of how to use the function: 

<<echo=TRUE, eval=TRUE>>=
datapath = "/home/mwl04747/RTricks/04_Regional_Climate_Trends/Data/SP24/"
downloadStations.fun(datapath, my.inventory)
@ 

This will download the data from the NOAA website and read it into R then written as csv files in your data folder.

\end{description}


Your are done with this guide, now go to Guide 2!

\section{Explaining the Code TBD!!}

\subsection{Trouble Shooting and Workarounds}

I will be getting all the guides working before working on this! But if there are errors with the custom function, this is where workarounds will be described! Please Slack me and mentors if you have any problems!

\subsection{Selecting Weather Records by State}

There are numerous ways to analyze temperature records, where stations can be analyzed individually or records could be sampled and analyzed in spatially in grids. Each of these are valid approaches depending on the question to be addressed. 

Here are the questions we will address: 

\begin{itemize}
  \item What stations have the longest meterological records in the USA?
  \item Can we determine the reliability of these stations?
  \item Finally, is there a temperature trend?
\end{itemize}


%\subsubsection{Identify List of State IDs (FIPS)}

%Using the rNOAA library in R, we can query NOAA's database to identify station codes (FIPS) by state. With the states and some territories, there are 55 FIPS for US weather stations. Althogh these include the District of Columbia, they do not include US Territories, sich as Guam, Puerto Rico, Marshall Islands, etc. We'll have to use a different search code for these.

%rNOAA has a simple function to list for each of the states and the weather stations in each. I use ncdc\_locs() functions to select each state and ncdc\_station() to obtain the station ids with the longest records. 

%The function queries the NOAA website and retrieves state codes, ``FIPS:XX''. Each state has a number of weather stations,\footnote{Project Idea: It would be nice to make a map of how concentrated the stations spatially.} some with a long record, some with a short record, and some with numerous interruptions. Our goal is to select a long record with few missing data. 

%\subsubsection{Selection Stations}

%With the state ids, we can evaluate the metadata for all the weather stations, which will work to get the longest records, using \texttt{ncdc\_stations()}. 

%First, we subset the data for stations that actively collecting data. Then we'll sort to the active stations to find the one with the longest records. We will use these stations for our analysis. 

%There were some records that didn't have robust TMAX/TMIN records, so there are some states that I had to manually select an alternative stations. 

\subsection{Defining Path \& Read Data}

First, we install some packages and read in the data. I suggest you create a folder for the project (I created one called ``04\_Regional\_Climate\_Trends") and then used the function here() to get the working directory and read the csv into R. This might be easier than the file.choose() option, but you can use that if you prefer.

<<>>=
library(here)
library(xtable)

stations.active.oldest = read.csv(
  here("04_Regional_Climate_Trends", "stations.active.oldest.csv"))

# OR
# use file.choose() to select the file
# filename = "MY.PATH/04_Regional_Climate_Trends/stations.active.oldest.csv"
# stations.active.oldest = read.csv(filename)
@ 


\subsection{Map US Weather Stations}

Here's a map of the weather stations in the dataset. Pretty lame map!  We'll make a better one later.\footnote{Evelyn/Brody: This is a good change to see how to use R for map making! First we need to transform that data.}

<<>>=
plot(stations.active.oldest$LONGITUDE, 
     stations.active.oldest$LATITUDE, 
     xlab = "Longitude",ylab = "Latitude", 
     pch=20, cex=0.3, col='gray60', las=1,
     main = "US Weather Stations")
@
\subsection{Select and Evaluate State Data}

<<>>=
stations.unique = 
  unique(stations.active.oldest[,c("STATE", "STATE_NAME")])

xtab = xtable(stations.unique)
@

The each of you will select a state -- see the Google Sheet sign up so we have a diverse set of states.

<<>>=
my.state = "CA" # change the "CA" to your state
@

\section{Download Data from NOAA}

\subsection{Subset Station Data by State}

This uses the stations.active.oldest file to download the data from the NOAA website based on the state you have choose.

The URL for the data is: \url{https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00023230/detail}.

<<>>=# Select Stations in State
my.stations = subset(stations.active.oldest, STATE == my.state)

# Download Updated Station Data
i=1
here::here("04_Regional_Climate_Trends", my.stations$ID[i])
@


% bibilography section here-------------------------------------------
%\clearpage

\bibliographystyle{apalike}
%\renewcommand\bibname{References}{}
\bibliography{/home/mwl04747/RTricks/references}%	\addcontentsline{toc}{chapter}{References}


\end{document}
