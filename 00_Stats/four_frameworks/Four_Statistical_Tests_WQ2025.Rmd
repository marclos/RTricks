---
title: "Four Statistical Tests and Four Statistical Framworks"
author: "EA030"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xtable)
```


## Test for Assocation

### Example #1: Polluters and Mobility

The third test is the Pearson correlation test. This test is used to test the association between two or more categories of count data. 


### Example #2: Polluters and Residential Mobility

```{r, echo=FALSE}
set.seed(78889)
# 2 x 2 contigency table
frequency = round(c(c(105+119)/2, c(93+150+97+179)/4, c(60+35+51)/3, 85), 0) #down, then over
dimnames = list(c( "Non-mobile", "Mobile"), c("Polluter", "Non-polluter"))
bubbles = as.table(matrix(frequency, nrow=2, dimnames=dimnames))
bubbles
```

```{r}
chisq.test(bubbles)
```

### Example #3: Cattle Grazing and Water Quality

Water quality is a key concern in areas where cattle grazing overlaps with recreational use. 
Previous studies (Derlet et al., 2008, *Water Quality Conditions Associated with Cattle Grazing and Recreation on National Forest Lands*) have shown that cattle presence is associated with elevated levels of *Escherichia coli* (E. coli), an indicator of fecal contamination.

This analysis tests whether there is a statistically significant association between the presence of cattle and *E. coli* detection in water samples.

---

#### Data

The following hypothetical dataset reflects water samples collected in two conditions: **sites with cattle present** and **sites without cattle**.  

| Cattle Presence | E. coli Detected | E. coli Not Detected | Total |
|-----------------|------------------|-----------------------|-------|
| Yes             | 28               | 12                    | 40    |
| No              | 10               | 30                    | 40    |
| **Total**       | 38               | 42                    | 80    |

---

#### R Code

```{r}
# Create contingency table
data <- matrix(c(28, 12,
                 10, 30),
               nrow = 2, byrow = TRUE)

colnames(data) <- c("Ecoli_Detected", "Ecoli_NotDetected")
rownames(data) <- c("Cattle_Present", "No_Cattle")
data <- as.table(data)

# Print table
data

# Perform Chi-square test
chisq.test(data)


```


## Regression Analysis

Fluorescence sensors provide a real-time, continuous monitoring method for detecting dissolved organic matter and microbial contamination in aquatic environments.  
In the study by Griffith et al. (2009), *Evaluation of real-time fluorescence sensors and benchtop fluorescence for tracking and predicting sewage contamination in the Tijuana River Estuary at the US-Mexico border*, regression analysis was applied to test whether fluorescence intensity could predict bacterial contamination indicators (e.g., *E. coli*, Enterococcus).

Here, we construct a regression model using simulated but realistic data to illustrate how fluorescence intensity relates to *E. coli* concentration.

---

### Example #4 Testing Reliability of a Method

Below is a simulated dataset representing paired measurements of fluorescence intensity (arbitrary units) and *E. coli* concentrations (log CFU/100 mL).  

| Sample | Fluorescence_Intensity | Ecoli_LogCFU |
|--------|-------------------------|--------------|
| 1      | 15.2                   | 2.1          |
| 2      | 18.5                   | 2.4          |
| 3      | 22.3                   | 2.9          |
| 4      | 25.1                   | 3.0          |
| 5      | 28.7                   | 3.4          |
| 6      | 30.2                   | 3.6          |
| 7      | 34.8                   | 3.9          |
| 8      | 37.5                   | 4.2          |
| 9      | 41.0                   | 4.5          |
| 10     | 45.2                   | 4.9          |

---

### R Code

```{r}
# Create dataset
fluorescence <- c(15.2, 18.5, 22.3, 25.1, 28.7, 30.2, 34.8, 37.5, 41.0, 45.2)
ecoli <- c(2.1, 2.4, 2.9, 3.0, 3.4, 3.6, 3.9, 4.2, 4.5, 4.9)

data <- data.frame(Fluorescence_Intensity = fluorescence,
                   Ecoli_LogCFU = ecoli)

# Fit linear regression model
model <- lm(Ecoli_LogCFU ~ Fluorescence_Intensity, data = data)

# Summary of model
summary(model)

# Plot relationship
plot(data$Fluorescence_Intensity, data$Ecoli_LogCFU,
     xlab = "Fluorescence Intensity (a.u.)",
     ylab = "E. coli (log CFU/100 mL)",
     main = "Regression: Fluorescence vs. E. coli")
abline(model, col = "blue", lwd = 2)

```



## Testing for Population Differences: t-test

### Example #4: Compost Maturity

The second test is the paired t-test. This test is used to compare the means of two groups. 

```{r}
medium = c(4, 2, 3.25, 3.25, 2.55, 2.34, 3.7, 3.7, 2.69, 2.77, 4.3, 5.2, 7.63, 7.68, 6, 6, 4.7, 4.5)
mature = c(2, 1, 2.65, 2.95, 2.26, 2.12, 5, 5.2, 2.85, 2.69, 3.1, 2.8, 7.64, 7.61, 3, 2.9)

par(mfrow=c(1,2), las=1)
hist(medium, main="Medium", xlab="Compost Maturity", col="lightblue")
hist(mature, main="Mature", xlab="Compost Maturity", col="lightgreen")
```

#### showing these in a density distribution and variablity

Boxplots are good ways of showing "categorical predictors"

```{r}
boxplot(medium, mature, names=c("Medium", "Mature"), 
      col=c("lightblue", "lightgreen"), ylab="Compost Maturity")
```


This is a bit of a sublety... 

```{r}
par(mfrow=c(1,2), las=1)
plot(density(medium), main="Medium", xlab="Compost Maturity", col="lightblue")
plot(density(mature), main="Mature", xlab="Compost Maturity", col="lightgreen")
```


#### R function for t-test

```{r}
  t.test(medium, mature, paired=FALSE)
```


#### Ploting the data using a normal distribution

Plotting Distributions -- why might this be useful?

```{r}
par(mfrow=c(1,2), las=1)
plot(dnorm(x=1:7, mean=mean(medium), sd=sd(medium)), main="Medium", xlab="Compost Maturity", ty="l", col="lightblue")
plot(dnorm(x=1:7, mean=mean(mature), sd=sd(medium)), main="Mature", xlab="Compost Maturity", ty="l", col="lightblue")

qqnorm(medium, main="Medium", col="lightblue")
qqline(medium, col="red")
qqnorm(mature, main="Mature", col="lightgreen")
qqline(mature, col="red")

```

## ANOVA

For this exercise, you will using the following data sets to analyze and interpret the results of four statistical tests and four statistical frameworks.

### Example #5:  Testing if Treatment Means are Equal

The first test is the Analysis of Variances (ANOVA) test. This test is used to compare the means of three or more groups. 

```{r}
treatments = c("A", "B", "C", "D")

```

#### Creating a Dataset

The data set tests the treatments of soil restoration in 'The Wash' and includes 10 replicated measurements for each treatment. 

To create the data set, use the following code that defines the treatments and generates the data for each treatment. The data is generated using the `rnorm` function. The `rnorm` function generates random numbers from a normal distribution. The `mean` and `sd` parameters are used to specify the mean and standard deviation of the normal distribution. The `rnorm` function is used to generate 10 random numbers for each treatment. The mean and standard deviation of the normal distribution are set to 10 and 2, respectively. 

```{r, echo=FALSE}
set.seed(4454)
rounding = 2
standarddeviation = 20
a = round(rnorm(10, mean = 9.3, sd = standarddeviation), rounding)
b = round(rnorm(10, mean = 8.5, sd = standarddeviation), rounding)
c = round(rnorm(10, mean = 9.0, sd = standarddeviation), rounding)
d = round(rnorm(10, mean = 9, sd = standarddeviation), rounding)
```

```{r, echo=FALSE}
values = c(a, b, c, d)
Treatments = rep(treatments, each = 10)
replicates = rep(1:10, 4)
anovadata = data.frame(Treatment = Treatments, 
              Replicate = replicates, Value = values)
```

```{r, results='asis'}
print(xtable(cbind(replicates, a, b, c, d)), type="latex")
```

#### Boxplot

The boxplot is used to visualize the data (simulated data).

```{r}
boxplot(Value ~ Treatment, data = anovadata, col = "lightblue", 
        main = "Boxplot of Treatment Means", 
        xlab = "Treatment", ylab = "Value")
```

#### Testing the Assumptions of ANOVA
  
The first step in the ANOVA test is to test the assumptions of the test. The assumptions of the ANOVA test are that the data is normally distributed and that the variances of the groups are equal.
  
The normality of the data is tested using the Shapiro-Wilk test. The Shapiro-Wilk test is used to test the null hypothesis that the data is normally distributed. The alternative hypothesis is that the data is not normally distributed. 

```{r}
summary(aov(Value ~ Treatment, data = anovadata))
```

The null hypothesis is that the means of the three species are equal. The alternative hypothesis is that the means of the three species are not equal. The ANOVA test is used to test the null hypothesis. 


### ANOVA Example #6: Arsenic Concentrations in Unregulated Water Sources


Elevated arsenic and uranium concentrations in unregulated water sources pose major public health challenges.  
Hoover et al. (2017), *Elevated Arsenic and Uranium Concentrations in Unregulated Water Sources on the Navajo Nation, USA*, highlighted that contaminant levels can differ across water sources such as wells, springs, and livestock tanks.  

Here, we use a **one-way ANOVA** to test whether **arsenic concentrations differ significantly by water source type**.  

---

#### Data

Simulated arsenic concentration data (in Âµg/L) across three source types:  

| Sample | SourceType    | Arsenic |
|--------|---------------|---------|
| 1      | Well          | 35.2    |
| 2      | Well          | 41.0    |
| 3      | Well          | 38.7    |
| 4      | Spring        | 12.5    |
| 5      | Spring        | 15.2    |
| 6      | Spring        | 10.8    |
| 7      | LivestockTank | 55.1    |
| 8      | LivestockTank | 61.3    |
| 9      | LivestockTank | 58.9    |

---

#### R Code

```{r}
# Simulated dataset
SourceType <- factor(c("Well","Well","Well",
                       "Spring","Spring","Spring",
                       "LivestockTank","LivestockTank","LivestockTank"))

Arsenic <- c(35.2,41.0,38.7,
             12.5,15.2,10.8,
             55.1,61.3,58.9)

data <- data.frame(SourceType, Arsenic)

# Run one-way ANOVA
anova_model <- aov(Arsenic ~ SourceType, data = data)

# Show results
summary(anova_model)

# Post-hoc pairwise comparisons
TukeyHSD(anova_model)
```


## Logistic Regression

The third test is the logistic regression test. This test is used to test the association between a binary response variable and one or more predictor variables. 


### Example #7: Distance and Success

Small square in tape on floor, 10x10 cm, between 20-300 cm, try to get in the square, by rolling or sliding the socket extension.

```{r, echo=FALSE}
set.seed(78889)
Distance = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10) + rnorm(20, 2, 1.5) 
Success = c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1)

group1 = c(95, 80, 55, 105, 200, 45, 305)
success1 = c(0, 1, 1, 0, 1,1, 0)
group2 = c(10, 20, 40, 100, 150, 250)
success2 = c(1, 1, 1, 1 , 1 , 0)
group3 = c(130, 65, 100, 120, 185, 240)
success3 = c(1, 1, 1, 0, 1, 1)
group4 = c(152, 92, 90, 95, 92, 215)
success4 = c(0, 1, 0, 1, 1, 0)
group5 = c(25, 33, 44, 105, 200, 222, 133, 243, 310)
success5 = c(1, 1, 1, 0, 0,0, 0, 0, 0)

mydata = data.frame(Distance = c(group1,group2, group3, group4, group5), Success = c(success1, success2, success3, success4, success5))

#mydata = data.frame(Distance, out)
mydata <- mydata[order(mydata$Distance),]
mydata$logical <- as.logical(mydata$Success)
head(mydata)
```


#### Plotting Data -- Linear Model

```{r}
str(mydata)

plot(mydata$Distance, mydata$Success, col="lightblue", pch=19, 
     ylab="Success", xlab="Distance")
abline(coef(lm(Success ~ Distance, data = mydata)), col="red", lwd=2)
```

#### Logistc Model

```{r}
mylogit <- glm(logical ~ Distance, data = mydata, family = "binomial")
summary(mylogit)

```
Plot Results
```{r}
plot(mydata$Distance, mydata$logical, col="lightblue", pch=19, ylab=c("Success"), xlab=c("Distance"), ylim=c(0,1),)
lines(mydata$Distance, fitted(mylogit), col="red", lwd=2)

```

### Example #8: Logistic Regression and Structural Violence in the Tijuana River

Vulnerable populations living in the Tijuana River canal face multiple forms of structural violence, including flooding risk, sewage exposure, and police harassment.  
Vogt (2018), *Deported, homeless, and into the canal: Environmental structural violence in the binational Tijuana River*, highlights how deported and homeless individuals are disproportionately exposed to hazardous environments.  

Here, we demonstrate the use of **logistic regression** to examine whether social and environmental factors (homelessness, deportation status, and proximity to the canal) predict the probability of **high exposure to risk**.

---

#### Data

The table below is a simulated dataset of 20 individuals. The outcome variable (`HighRisk`) is binary (1 = high environmental exposure risk, 0 = low risk). Predictors include:  
- `Homeless` (1 = yes, 0 = no)  
- `Deported` (1 = yes, 0 = no)  
- `NearCanal` (Distance in Meters)  

| ID | Homeless | Deported | NearCanal | HighRisk |
|----|----------|----------|-----------|----------|
| 1  | 1        | 1        | 2         | 1        |
| 2  | 1        | 1        | 4         | 1        |
| 3  | 1        | 0        | 3         | 1        |
| 4  | 1        | 1        | 26         | 1        |
| 5  | 0        | 1        | 1.3         | 1        |
| 6  | 0        | 1        | 30         | 0        |
| 7  | 1        | 0        | 42         | 0        |
| 8  | 0        | 0        | 2.1         | 0        |
| 9  | 0        | 0        | 4         | 0        |
| 10 | 1        | 1        | 2         | 1        |
| 11 | 0        | 1        | 5         | 1        |
| 12 | 1        | 0        | 2         | 1        |
| 13 | 1        | 1        | 59         | 1        |
| 14 | 0        | 0        | 80         | 0        |
| 15 | 0        | 0        | 3         | 0        |
| 16 | 1        | 1        | 7         | 1        |
| 17 | 0        | 1        | 12         | 0        |
| 18 | 1        | 0        | 19         | 0        |
| 19 | 0        | 1        | 14         | 1        |
| 20 | 1        | 1        | 2         | 1        |

---

#### R Code

```{r}
# Simulated dataset
ID <- 1:20
Homeless <- c(1,1,1,1,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,1)
# Deported <- c(1,1,0,1,1,1,0,0,0,1,1,0,1,0,0,1,1,0,1,1)
NearCanal <- c(2,4,3,26,1.3,30,42,2.1,4.0,2,5,2,50,80,3,13,12,29,24,2)
HighRisk <- c(1,1,1,0,1,0,0,1,0,1,1,1,1,0,0,1,0,0,0,1)

data <- data.frame(ID, Homeless, NearCanal, HighRisk)
data <- data[order(data$NearCanal),]
```

#### Data Analysis
```{r}
# Logistic regression
model <- glm(HighRisk ~ NearCanal, 
             family = binomial(link="logit"), data = data)

summary(model)

# Convert to odds ratios
exp(cbind(OR = coef(model), confint(model)))
```

```{r}
plot(data$NearCanal, data$HighRisk, col="lightblue", pch=19, ylab=c("Urgent Care"), xlab=c("NearCanal"), ylim=c(0,1),)
lines(data$NearCanal, fitted(model), col="red", lwd=2)
```

