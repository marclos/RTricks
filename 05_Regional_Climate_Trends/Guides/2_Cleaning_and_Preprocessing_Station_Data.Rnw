\documentclass{article}
\usepackage{natbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=purple,
    citecolor=violet
}

\usepackage[dvipsnames]{xcolor}
\colorlet{LightRubineRed}{RubineRed!100}
\definecolor{mypink2}{RGB}{219, 48, 122}

\title{Guide 2: Cleaning and Pre-Processing Weather Station Data}
\author{Marc Los Huertos}
\date{\today~(ver. 0.99)} 

\begin{document}
\maketitle

\section{Introduction}

\subsection{Goals}

The goal of this guide is to provide a step-by-step process for cleaning and pre-processing weather station data. The data is from the Global Historical Climatology Network (GHCN) Daily dataset. The data is available from the National Oceanic and Atmospheric Administration (NOAA) and is available from the National Centers for Environmental Information (NCEI) at \url{https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily}.

\subsection{Background}

\subsubsection{What is GHCN-Daily (GNCN-d)?}

The new dataset is referred to as GHCN-Daily, version 3. The new dataset has a number of improvements over the older dataset, including a more comprehensive and robust quality assurance process. The new dataset also includes more station records and is updated more frequently than the older dataset. 

The new dataset is also available in a more user-friendly format, including a set of pre-packaged data files that are available for download from the NCEI website. However, devoloping instructions to interact with their website is way to time consuming! So, we'll use the R file transfer protocol, via https, to download the data. 

In 2011, there was a major update to the daily updates to the GHCN dataset: \href{https://www.ncei.noaa.gov/pub/data/ghcn/daily/COOPDaily_announcement_042011.pdf}{IMMEDIATE -- Changes to COOP Daily Form Access}. It's a useful history.

\subsection{Approach}

We need to address several things that might get in the way of our analysis: 

\begin{enumerate}
  \item Read csv files into R (from station.csv files)
  \item Fix date format (convert to POSIXct, adding month and year as variables)
  \item Convert units (VALUE: PRCP = 0.1 mm, TMAX/TMIN = 0.1 C)
  \item Evaluate missing data (Coverage.fun)
  \item Evaluate for Outliers (QACQ.fun)
  \item Other stuff?? We will see what we need to do as we go along. The data evolve each year.
\end{enumerate}

\section{Cleaning and Pre-Processing Functions}
<<setup, echo=FALSE, results='hide', message=FALSE>>=
library(here)
source(here("05_Regional_Climate_Trends", "Guides", "Guide2functions.R"))
@

\subsection{Before Starting the Process}

Before you begin, make sure you have the stations to read into R. Got to the ``Files" tab in RStudio and make sure you see the station csv files in your data folder. If not, please Slack Marc and mentors, so we can troubleshoot the issue!

\subsection{R Code with Custom Functions}

From the Canvas page, go to the Guide2functions.R file and download the file to your computer. Then upload the file to Rstudio directory you are using for the project.

Open the file in Rstudio and run the code, using the ``source''. button near the top of the editor window.

Run the \href{https://github.com/marclos/RTricks/blob/master/05_Regional_Climate_Trends/Guides/Guide2functions.R}{Guide2functions.R} code and the functions will be loaded into your environment automatically. Be sure the function has been updated to 2025-02-01!

\subsection{Function Descriptions and Use}

The following custom functions are used to read the weather stations data into R, clean and pre-process data so we can analyze the data with the next guide.

\subsection{Reading csv and Checking dataframe objects}

\begin{description}

\item[Read all csv files into R]

The data are now ready to be read into R environment, we will read them in using the following function: 

\begin{center}
\textbf{Function: \textcolor{Plum}{ReadStations2.fun()}}
\end{center}

Use this function read the stations in the R Environment. 

Example of how to use the function:

<<readstations, echo=TRUE, results='hide'>>=
datafolder = "/home/mwl04747/RTricks/05_Regional_Climate_Trends/Data/SP25/"
ReadStations2.fun(datafolder)
@

\noindent Remember that datafolder is a path of hierarchical folders and should end with ".../Data/" for your path. Why is mine different? Becuase I often have to help students and want to save the data in a unique path for each spring course.

\item[List the objects in the R environment] The ls() function will list the objects in the R environment. Use this in the console. Notice nothing goes inside 
the parethesis. 

<<echo=TRUE>>=
ls()
@

\noindent What do you see? Your list of objects should include several things like mine: some weather stations,  You should see objects named after the stations that have been read into R. In addition, you should see several functions (.fun) and other miscilenous objects. If so, you have been making progress. If you don't see something parallel to my list, let me or mentors know so we can trouble shoot, or look at section~\ref{sec:troubleshooting}. 

% (e.g. %\Sexpr{my.inventory$ID[1]}, you should see an object named that way into your R environment.

\item[Check the structure of the dataframes]

By using \texttt{str()}, we can ensure the datasets look right!

The syntax requires an object, in our case, the object name for a weather stations's data. For example: 

<<stationstructure, echo=FALSE, results='markup'>>=
str(USC00042294)
@

What to look for?  Is the object a data.frame?  Does it have the right number of observations? Are the expected variable names present?  Are the data types correct?  Are values in each variable reasonable?

\item[Sorting Stations by Number of Observations -- Skip This]

In theory, the stations with the greatest number of observations are the the ones we will want to evaluate. However, we may also want to evaluate stations to cover a geographic range. 

%For now, let use this function to sort the stations by the number of observations and use this to focus are attention. 

THIS IS A BRAND NEW IDEA FROM WEDNESDAY'S LAB, AND WILL WORK ON THIS FOR NEXT SEMESTER.  Instead look at the number of observation in the R environment to select 3 stations with the highest number of observations. 

\begin{center}
\textbf{Function: \textcolor{Plum}{sortstations.fun()}}
\end{center}

Example of how to use the function:

<<sortstations, echo=TRUE, results='hide'>>=
# sortstations.fun() Not working yet. 
@

\end{description}

\subsection{Clean Data}

Next we'll ``clean'' the dataset by fixing the date format and preparing it for the analysis stages. I suggest you select two stations with the highest number of observations to work with. We will know if this is a problem soon enough and if it is, we can use one of the 13 remaining stations, if your state/terriborty has that many to choose from. 

\begin{description}

\item[Function to Fix Dates] This function converts date values to a format that R can understand as a date, so we can, among other things, create montly means.

\begin{center}
\textbf{Function: \textcolor{Plum}{fixDates.fun()}}
\end{center}

Example of how to use the function (Note: the new dataframe has a name change with an ``a'' following the station. This is so we can keep the original data and the processed data separate.): 

<<fixdates, echo=TRUE, results='hide'>>=
USC00042294a <- fixDates.fun(USC00042294)
USC00040693a <- fixDates.fun(USC00040693)
@

\item[Evaluation Temporal Coverage]

We need to know how much data we have for each station. This is important for the next steps in the process.

\begin{center}
\textbf{Function: \textcolor{Plum}{coverage.fun()}}
\end{center}

Example of how to use the function: 

<<checkcoverage, echo=TRUE, results='hide'>>=
coverage.fun(USC00042294a)
coverage.fun(USC00040693a)
@

In general, we want something like 95\% coverage for the period of record. If you don't have that, please let us know and we'll help you get additional stations with better coverage!

See Section~\ref{subsec:coverage} for more information on how to evaluate the coverage of the data.

\item[Function to Fix Values (by order of magnitude)]

According the the NOAA website, the csv.gz files have  five core elements include the following units, which we will convert:

\begin{description}
  \item[PRCP =] Precipitation (tenths of mm) $\rightarrow$ mm
  \item[SNOW =] Snowfall (mm)  $\rightarrow$ cm
	\item[SNWD =] Snow depth (mm) $\rightarrow$ cm
  \item[TMAX =] Maximum temperature (tenths of degrees C) $\rightarrow$ degrees C
  \item[TMIN =] Minimum temperature (tenths of degrees C) $\rightarrow$ degrees C
\end{description}

\begin{center}
\textbf{Function: \textcolor{Plum}{fixValues.fun()}}
\end{center}

Example of how to use the function (Note: the new dataframe has a name change with a ``b'' following the station. This is so we can keep the original data and the processed data separate.):

<<fixValues, echo=TRUE, results='hide'>>=
USC00042294b <- fixValues.fun(USC00042294a)
USC00040693b <- fixValues.fun(USC00040693a)
@

\item[Checking for Outliers]

NOAA website conducts a very rudimentary data quality check, but every year, we find stations with wacky numbers. Hopefully this function will find them. But if you do have some, let Marc and mentors know so we can figure out how to address them!

Here are some stuff we'll look for: 
\begin{description}
  \item[Extreme Values] Plot values with time, is the scale crazy with just a few observations at the extreme?
  \item[Sudden Shift] A sudden data shift in GHCNd (Global Historical Climatology Network - Daily) could indicate several potential issues or changes in the dataset. Some possible explanations include:  

\begin{enumerate}  
    \item \textbf{Instrument Changes or Malfunctions} â€“ A shift might result from a change in measurement instruments, sensor recalibration, or instrument failure, leading to discontinuities in recorded values.  

    \item \textbf{Station Relocation} â€“ If a weather station is moved, even slightly, it can cause abrupt changes due to differences in elevation, local microclimates, or exposure.  

    \item \textbf{Changes in Observation Practices} â€“ Adjustments in how data is recorded, such as shifts in time-of-day observation or methodology (e.g., moving from manual readings to automated sensors), can introduce noticeable changes.  

    \item \textbf{Data Quality Issues or Corrections} â€“ The dataset may have been updated to correct errors, remove outliers, or introduce new quality control procedures, affecting trends.  

    \item \textbf{Climatic or Extreme Weather Events} â€“ A real, significant climate event, such as a heatwave, cold snap, or precipitation anomaly, could explain the sudden shift.  

    \item \textbf{Urbanization or Land Use Changes} â€“ Increased urbanization around a station (e.g., heat island effects) or changes in land use (e.g., deforestation) can cause long-term shifts in climate data.  

    \item \textbf{Metadata Errors or Missing Data Fill-ins} â€“ If there are discrepancies in metadata or if missing data is interpolated with estimates, sudden shifts can appear.  
\end{enumerate} 

\end{description}

If you notice a sudden shift, it's worth cross-referencing station metadata, quality control flags, and surrounding station records to determine whether the change is due to an actual climatic event or a data-related issue.

  \item[QA/QC Flags in GHCNd]

In the GHCNd dataset, Quality Assurance (QA) and Quality Control (QC) flags are used to indicate potential issues or modifications in the data. These flags help users identify values that may be erroneous, adjusted, or estimated.  

\begin{itemize}  
    \item \textbf{Types of QA/QC Flags:} Each reported data value in GHCNd can have three associated flags:  
    \begin{itemize}  
        \item \textbf{Measurement Flag} â€“ Indicates if the data value was derived, adjusted, or estimated.  
        \item \textbf{Quality Flag} â€“ Identifies if a value failed a quality control check.  
        \item \textbf{Source Flag} â€“ Specifies the source of the data.  
    \end{itemize}  
\end{itemize}  

\begin{itemize}  
    \item \textbf{Measurement Flags (Optional):} Indicate if the data value was derived, estimated, or adjusted. Some common flags include:  
\end{itemize}  

\begin{table}[h]  
    \centering  
    \begin{tabular}{|c|l|}  
        \hline  
        \textbf{Flag} & \textbf{Meaning} \\  
        \hline  
        B & Adjusted to remove systematic bias (e.g., urban heat island effects) \\  
        D & Data value was from a delayed source \\  
        E & Estimated value \\  
        I & Derived from an intermediate source \\  
        M & Missing data \\  
        Q & Data value adjusted from original observation \\  
        S & Value was computed from multiple sources \\  
        T & Trace precipitation (small, nonzero amount) \\  
        \hline  
    \end{tabular}  
    \caption{Measurement Flags in GHCNd}  
\end{table}  

\begin{itemize}  
    \item \textbf{Quality Control (QC) Flags:} Indicate if a value has failed a quality check. If no QC flag is present, the value has passed all checks. Common QC flags include:  
\end{itemize}  

\begin{table}[h]  
    \centering  
    \begin{tabular}{|c|l|}  
        \hline  
        \textbf{Flag} & \textbf{Meaning} \\  
        \hline  
        D & Failed duplicate check \\  
        G & Failed gap check \\  
        I & Failed internal consistency check \\  
        K & Failed streak/frequent-value check \\  
        L & Failed check on length of multiday period \\  
        M & Failed climatological outlier check \\  
        N & Failed plausible value check \\  
        O & Failed observational consistency check \\  
        S & Failed spatial consistency check \\  
        T & Failed temporal consistency check \\  
        W & Temperature too warm for snow \\  
        \hline  
    \end{tabular}  
    \caption{Quality Control Flags in GHCNd}  
\end{table}  

\begin{itemize}  
    \item \textbf{Source Flags:} Indicate the origin of the data, such as manual observations, automated weather stations, or third-party sources. Common flags include:  
\end{itemize}  

\begin{table}[h]  
    \centering  
    \begin{tabular}{|c|l|}  
        \hline  
        \textbf{Flag} & \textbf{Meaning} \\  
        \hline  
        0-9  & Climate Reference Network (CRN) source code \\  
        C    & Data from the Cooperative Observer Network (COOP) \\  
        G    & Data from the Global Summary of the Day (GSOD) \\  
        H    & Data from hydrological stations \\  
        R    & Data from air force stations \\  
        S    & Data from satellite-derived observations \\  
        Z    & Data from summary statistics \\  
        \hline  
    \end{tabular}  
    \caption{Source Flags in GHCNd}  
\end{table}  

\begin{itemize}  
    \item \textbf{Interpreting GHCNd QA/QC Flags:}  
    \begin{itemize}  
        \item If a value \textbf{has no QC flag}, it has passed all quality checks.  
        \item A value with a \textbf{measurement flag "E"} means it was estimated and should be used with caution.  
        \item A QC flag like \textbf{"M" (outlier check failed)} suggests the value may be erroneous.  
        \item The \textbf{source flag} helps track where the data originated.  
    \end{itemize}  
\end{itemize}  

If you notice unexpected flags, consider cross-referencing metadata, station history, and nearby station records to determine whether the flagged values indicate a real climatic event or a data issue.



\begin{center}
\textbf{Function: \textcolor{Plum}{QAQC.fun()}}
\end{center}

At this point, the function produced graphics and a summary of flags to examine the potential for QA/QC problems -- in part becuase the issues seem rare. Maybe this will be the year where someone finds a problem that we need to address and I'll revise the function accordingly. 

Example of how to use the function:

<<echo=TRUE, results='markup'>>=
QAQC.fun(USC00042294b)
QAQC.fun(USC00040693b)
@

No flags are reportedhere. If you have any hints of data problems (wacky values, odd patterns), let's sort them out together!

\item[Function to Create Monthly Values and Normals]

For TMAX and TMIN, we want monthly means, for rainfall, we'll want monthly totals.\footnote{Brody: We need code to exclude months with missing data, these might not be represenative of the month if missing, especically for PRCP!}

\begin{center}
\textbf{Function: \textcolor{Plum}{MonthlyValues.fun()}}
\end{center}

\begin{center}
\textbf{Function: \textcolor{Plum}{NormalValues.fun()}}
\end{center}

Example of how to use the functions:

<<monthymeans_normals, echo=TRUE, results='hide'>>=
USC00042294.monthly <- MonthlyValues.fun(USC00042294b)
USC00042294.normals <- MonthlyNormals.fun(USC00042294b)
USC00040693.monthly <- MonthlyValues.fun(USC00040693b)
USC00040693.normals <- MonthlyNormals.fun(USC00040693b)
@

\item[Function to Create Anomalies]

Example of how to use the function:

<<anomalies, echo=TRUE, results='hide'>>=
USC00042294.anomalies <- 
  MonthlyAnomalies.fun(USC00042294.monthly, USC00042294.normals)
USC00040693.anomalies <- 
  MonthlyAnomalies.fun(USC00040693.monthly, USC00040693.normals)
@

\end{description}

\subsection{Checking on the Results}

You can double check that the dataframes you have been making are actually present by using the ls() function the console again. 

<<listnewdataframes>>=
ls()
@

Getting into the data is a bit tricky. The datasets is a list of dataframes. Each dataframe is a different variable, where 1 is TMAX, 2 is TMIN, and 3 is PRCP.

The get access to each you use the following code:

\begin{itemize}
  \item \verb|USC00042294.anomalies[[1]]| for TMAX
  \item \verb|USC00042294.anomalies[[2]]| for TMIN
  \item \verb|USC00042294.anomalies[[3]]| for PRCP
\end{itemize}

\begin{figure}
<<plotanomalies, echo=FALSE>>=
# plot the data
par(mfrow=c(3,1), mar=c(4,5,3,2))
plot(TMAX.a ~ Ymd, data=USC00042294.anomalies[[1]], type="p", col="red", pch=19, cex=.2, las=1, xlab="Year", ylab="Temperature Anamoly (C)", main="Monthly Maximum Temperature Change")
plot(TMIN.a ~ Ymd, data=USC00042294.anomalies[[2]], type="p", col="blue", pch=19, cex=.2, las=1, xlab="Year", ylab="Temperature Anamoly (C)", main="Monthly Minimum Temperature Change")
plot(PRCP.a ~ Ymd, data=USC00042294.anomalies[[3]], type="p", col="green", pch=19, cex=.2, las=1, xlab="Year", ylab="Precipitation Anamoly (C)", main="Monthly Precipiation Change")
@
\caption{Here's a quick test to see if my data are coming out as expected and as a preview.}

%You can think of this as simply \verb plot(y~x, data=station)and build from there if you are interested.}
\end{figure}

\textcolor{Periwinkle}{\Large Your are done with this guide, now take a break, a walk, and enjoy some screen free downtime. Next, go to Guide 3!}

\clearpage

\subsection{Clean Up R Environment}

I tend to avoid having lots of objects in my R environment. I like to clean up after myself. 

Also, I like to move the final products into csv files. Here's the function to do that. However, I noticed that it' deleting needed files. YIKES! It's also clunky, but not that important at this point, nevertheless, I'll work on it later.So, I suggest you don't run this code until I fix it.

<<cleanup, echo=TRUE, eval=FALSE>>=
CleanUp.fun(datafolder, USC00042294.anomalies, "USC00042294")
@

For now, I am just saving the anomalies data into an RData file that I can use in other Rmd Guides by loading. I doubt you'll need to do that if you use only one Rmd file to knit all the functions together.

<<saveRdata>>=
SaveCleanUp.fun(datafolder)
@

\clearpage

\section{Describing Marc's Custom Functions}

\subsection{ReadStations2.fun}

<<show_ReadStation.fun, echo=FALSE>>=
print(ReadStations2.fun)
@


\subsubsection{Trouble Shooting}

The function fails to read some csv files into the R environment for a variety of reasons that I haven't been able to solve. 

To work around the issues, we'll have to read the csv filed directly. Here's how an example of how we might do it:

\begin{verbatim}
USC00042294 <- read.csv("data/USC00042294.csv")
\end{verbatim}


\subsection{fixdates.fun}

<<show_fixdates.fun, echo=FALSE>>=
print(fixDates.fun)
@

\subsection{ConvertUnits.fun}

<<show_ConvertUnits.fun, echo=FALSE>>=
print(fixValues.fun)
@


\subsection{QAQC.fun}

<<show_QAQC.fun, echo=FALSE>>=
print(QAQC.fun)
@

\subsubsection{How to Evaluate QA/QC Problems}

\subsubsection{QA/QC Trouble Shooting}

I will be getting all the guides working before working on this! But if there are errors with the custom function, this is where workarounds will be described! Please Slack me and mentors if you have any problems!

\subsubsection{Coverage Problems ($<$95\%)}\label{subsubsec:coverage}

<<show_datacoverage.fun, echo=FALSE>>=
print(coverage.fun)
@

If you have too many stations that don't have enough data, we'll need to download additional stations. I can show you a way to decipher that ahead of time if you'd like to know. Otherwise, I suggest you double the number of stations selected in the code that generated my.inventory. I have increase the number of stations per state to 15, so perhaps that will give you enough to work with.

If you get an error with this function, be sure you are using the correct file -- in our case, it should end with an ``a''. If you are using the wrong file, you'll get an error.

\subsubsection{Missing Data}

Similar to the problem above, missing data can be a problem. The real issue that I can see is that rainfall is so central because we use the data to calculate monthly totals, but if the month is missing data, then we have a severe bias. Same issue in temperature, but because we are looking at averages, it's less likely to be a major source of bias. But we should should check!

\subsection{Failure to Read csv files}

I noticed that the read.csv function is not working for some files. I'm not sure why, but I suspect it's because the files are too large. I'll work on this later. For now, let me know if you have any issues with this.

\subsubsection{Plot Anomaly}

Graphic has lots of issues. more next time!  But here's a start.

<<quickplotcheck, eval=FALSE>>=
options(scipen=5)
par(mar=c(4,6,2,5))

plot(TMAX.a ~ YEAR, data = subset(, MONTH == 1), 
     las=1, pch=19, col = "blue", cex=.5, #xlab = "Year", 
     ylab = "Maximum Temp Anomaly (C)",
     main="January Maximum Temp Anomaly")
mtext("Maximum Temp Anomaly (C)", side = 2, line = 3)
temp.lm = lm(ANOMALY ~ YEAR, data = subset(station1.TMAX, MONTH == 1))
abline(coef(temp.lm), col = "red")
@

We can see huge periods of time where no data was collecged. Yikes! I don't think I can use this station.

My custom functions are probably sensitive to missing values, need to work on that!

\subsection{MonthlyValues.fun}

Here's the function for Monthly Normals:

<<pringMonthlyNormals.fun, echo=FALSE>>=
print(MonthlyNormals.fun)
@

If you try to submit the wrong station file, e.g. `USC00042294`, you'll get an error. You need to submit the file that ends with an "b".

Here's the function for Monthly Values: 

<<printMonthlyValues.fun, echo=FALSE>>=
print(MonthlyValues.fun)
@

If you try to submit the wrong station file, e.g. `USC00042294`, you'll get an error. You need to submit the file that ends with an "b".

\subsection{MonthlyAnomalies.fun}

Here's the function: 

<<printMonthlyAnomalies.fun, echo=FALSE>>=
print(MonthlyAnomalies.fun)
@

If this function fails, it's likely because the products from the 
MonthlyNormals.fun or MonthlyValues.fun functions did not work or the products of these functions are not in the R environment and correctly specified in the function call. 


\section{Trouble Shooting and Work Arounds}\label{sec:troubleshooting}

\subsection{Checking Results Step by Step}

I generally check every step of the way to make sure the function is working. You can look the ``Global Environment" in RStudio to see if the obects are there. 

In addition, I have written some custom functions to evaluate the dataframes we have (hopefully) created!

\begin{itemize}

\item Does these object exist?

<<dfexists, eval=TRUE>>=
df_exists("USC00042294b")
@

\item What are the names within the dataframe?
<<df_names, echo=TRUE>>=
df_names(USC00042294b)
@

\item What are the first few rows of the dataframe?
<<df_head, echo=TRUE>>=
df_head(USC00042294.anomalies)
@


\end{itemize}

\section{Next Steps}

\subsection{Apply Function to All Stations}

So far, I have only run function for 1 station, but I suspect you can figure out how to run it for each one!

This is all we need to do so far. Next week, we'll look at different way to visualize the data! 

\subsection{Save and Clean Up R Environment}

I'll save all the station data into csv files, then use them in the next guide to clean, process, and visualize data. I don't think the function is all that useful, so I can show you better ways of doing thin is class.

<<saveclieanup, echo=TRUE>>=
SaveCleanUp.fun(datafolder)
@

<<cleandataframe, eval=FALSE>>=
station1.clean=cleandataframe.fun(station1)
@


\end{document}
